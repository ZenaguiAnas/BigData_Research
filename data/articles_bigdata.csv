Title,Authors,HTML Link,PDF Link,Abstract,Published in,Date of Publication,Page(s),Electronic ISSN,PubMed ID,DOI,DOI Link,Publisher
Random Sample Partition: A Distributed Data Model for Big Data Analysis,Salman Salloum; Joshua Zhexue Huang; Yulin He; ; ; ,https://ieeexplore.ieee.org/document/8706990/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8706990,"With the ever-increasing volume of data, alternative strategies are required to divide big data into statistically consistent data blocks that can be used directly as representative samples of the entire data set in big data analysis. In this paper, we propose the Random Sample Partition (RSP) distributed data model to represent a big data set as a set of disjoint data blocks, called RSP blocks. Each RSP block has a probability distribution similar to that of the entire data set. RSP blocks can be used to estimate the statistical properties of the data and build predictive models without computing the entire data set. We demonstrate the implications of the RSP model on sampling from big data and introduce a new RSP-based method for approximate big data analysis which can be applied to different scenarios in the industry. This method significantly reduces the computational burden of big data and increases the productivity of data scientists.",IEEE Transactions on Industrial Informatics,06 May 2019,5846 - 5854,,,10.1109/TII.2019.2912723,https://doi.org/10.1109/TII.2019.2912723,IEEE
Distributed Data Strategies to Support Large-Scale Data Analysis Across Geo-Distributed Data Centers,Tamer Z. Emara; Joshua Zhexue Huang; ; ,https://ieeexplore.ieee.org/document/9208652/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9208652,"As the volume of data grows rapidly, storing big data in a single data center is no longer feasible. Hence, companies have developed two scenarios to store their big data in multiple data centers. In the first scenario, the company's big data are distributed in multiple data centers without data replication. In the second scenario, data are also stored in multiple data centers but important data are replicated in these data centers to increase data safety and availability. However, in these scenarios, analyzing big data distributed in multiple data centers becomes a challenging task. In this paper, we propose two data distribution strategies to support big data analysis across geo-distributed data centers. In these strategies, we use the recent Random Sample Partition data model to convert big data into sets of random sample data blocks and distribute these data blocks into multiple data centers either without replication or with replication. In analyzing big data in multiple data centers without replication, we randomly select samples of data blocks from multiple data centers and download the sample data blocks to one data center for analysis. In the second strategy with replication of data blocks, we can analyze big data on any data center by randomly selecting a sample of data blocks replicated from other data centers. This strategy avoids data transformation between data centers. We demonstrate the performance of the two strategies in big data analysis by using simulation results produced on one local data center and four AWS data centers in North America, Asia, and Australia.",IEEE Access,29 September 2020,178526 - 178538,2169-3536,,10.1109/ACCESS.2020.3027675,https://doi.org/10.1109/ACCESS.2020.3027675,IEEE
Big Data for Remote Sensing: Challenges and Opportunities,Mingmin Chi; Antonio Plaza; Jón Atli Benediktsson; Zhongyi Sun; Jinsheng Shen; Yangyong Zhu; ; ; ; ; ; ,https://ieeexplore.ieee.org/document/7565634/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7565634,"Every day a large number of Earth observation (EO) spaceborne and airborne sensors from many different countries provide a massive amount of remotely sensed data. Those data are used for different applications, such as natural hazard monitoring, global climate change, urban planning, etc. The applications are data driven and mostly interdisciplinary. Based on this it can truly be stated that we are now living in the age of big remote sensing data. Furthermore, these data are becoming an economic asset and a new important resource in many applications. In this paper, we specifically analyze the challenges and opportunities that big data bring in the context of remote sensing applications. Our focus is to analyze what exactly does big data mean in remote sensing applications and how can big data provide added value in this context. Furthermore, this paper describes the most challenging issues in managing, processing, and efficient exploitation of big data for remote sensing problems. In order to illustrate the aforementioned aspects, two case studies discussing the use of big data in remote sensing are demonstrated. In the first test case, big data are used to automatically detect marine oil spills using a large archive of remote sensing data. In the second test case, content-based information retrieval is performed using high-performance computing (HPC) to extract information from a large database of remote sensing images, collected after the terrorist attack to the World Trade Center in New York City. Both cases are used to illustrate the significant challenges and opportunities brought by the use of big data in remote sensing applications.",Proceedings of the IEEE,13 September 2016,2207 - 2219,,,10.1109/JPROC.2016.2598228,https://doi.org/10.1109/JPROC.2016.2598228,IEEE
Cybersecurity in Big Data Era: From Securing Big Data to Data-Driven Security,Danda B. Rawat; Ronald Doku; Moses Garuba; ; ; ,https://ieeexplore.ieee.org/document/8673585/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8673585,"‘‘Knowledge is power” is an old adage that has been found to be true in today’s information age. Knowledge is derived from having access to information. The ability to gather information from large volumes of data has become an issue of relative importance. Big Data Analytics (BDA) is the term coined by researchers to describe the art of processing, storing and gathering large amounts of data for future examination. Data is being produced at an alarming rate. The rapid growth of the Internet, Internet of Things (IoT) and other technological advances are the main culprits behind this sustained growth. The data generated is a reflection of the environment it is produced out of, thus we can use the data we get out of systems to figure out the inner workings of that system. This has become an important feature in cybersecurity where the goal is to protect assets. Furthermore, the growing value of data has made big data a high value target. In this paper, we explore recent research works in cybersecurity in relation to big data. We highlight how big data is protected and how big data can also be used as a tool for cybersecurity. We summarize recent works in the form of tables and have presented trends, open research challenges and problems. With this paper, readers can have a more thorough understanding of cybersecurity in the big data era, as well as research trends and open challenges in this active research area.",IEEE Transactions on Services Computing,25 March 2019,2055 - 2072,,,10.1109/TSC.2019.2907247,https://doi.org/10.1109/TSC.2019.2907247,IEEE
A Big Data-as-a-Service Framework: State-of-the-Art and Perspectives,Xiaokang Wang; Laurence T. Yang; Huazhong Liu; M. Jamal Deen; ; ; ; ,https://ieeexplore.ieee.org/document/8053816/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8053816,"Due to the rapid advances of information technologies, Big Data, recognized with 4Vs characteristics (volume, variety, veracity, and velocity), bring significant benefits as well as many challenges. A major benefit of Big Data is to provide timely information and proactive services for humans. The primary purpose of this paper is to review the current state-of-the-art of Big Data from the aspects of organization and representation, cleaning and reduction, integration and processing, security and privacy, analytics and applications, then present a novel framework to provide high-quality so called Big Data-as-a-Service. The framework consists of three planes, namely sensing plane, cloud plane and application plane, to systemically address all challenges of the above aspects. Also, to clearly demonstrate the working process of the proposed framework, a tensor-based multiple clustering on bicycle renting and returning data is illustrated, which can provide several suggestions for rebalancing of the bicycle-sharing system. Finally, some challenges about the proposed framework are discussed.",IEEE Transactions on Big Data,29 September 2017,325 - 340,,,10.1109/TBDATA.2017.2757942,https://doi.org/10.1109/TBDATA.2017.2757942,IEEE
An Asymptotic Ensemble Learning Framework for Big Data Analysis,Salman Salloum; Joshua Zhexue Huang; Yulin He; Xiaojun Chen; ; ; ; ,https://ieeexplore.ieee.org/document/8586790/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8586790,"In order to enable big data analysis when data volume goes beyond the available computing resources, we propose a new method for big data analysis. This method uses only a few random sample data blocks of a big data set to obtain approximate results for the entire data set. The random sample partition (RSP) distributed data model is used to represent a big data set as a set of non-overlapping random sample data blocks. Each block is saved as an RSP data block file that can be used directly to estimate the statistical properties of the entire data set. A subset of RSP data blocks is randomly selected and analyzed with existing sequential algorithms in parallel. Then, the results from these blocks are combined to obtain ensemble estimates and models which can be improved gradually by appending new results from the newly analyzed RSP data blocks. To this end, we propose a distributed data-parallel framework (Alpha framework) and develop a prototype of this framework using Microsoft R Server packages and Hadoop distributed file system. The experimental results of three real data sets show that a subset of RSP data blocks of a data set is sufficient to obtain estimates and models which are equivalent to those computed from the entire data set.",IEEE Access,23 December 2018,3675 - 3693,2169-3536,,10.1109/ACCESS.2018.2889355,https://doi.org/10.1109/ACCESS.2018.2889355,IEEE
Research on the Impact of Big Data Capabilities on Government’s Smart Service Performance: Empirical Evidence From China,Airong Zhang; Na Lv; ; ,https://ieeexplore.ieee.org/document/9344686/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9344686,"The government of China seeks to improve e-government service quality and build a service-oriented government that citizens find satisfactory. To this end, big data is being used as a new tool of government service innovation. However, there is a lack of research on how big data affects the performance of government smart services. This article explores the influence mechanisms of government big data capabilities on the performance of smart service provision, utilizing the carding analysis of relevant literature, published both in China and abroad. To this end, a structural equation model was constructed. Using data from 289 valid questionnaires in Jiangsu, Shandong, Zhejiang, and other provinces and cities in China, the study tests internal mechanisms of big data capabilities and its effect on smart service performance. Following a new definition of government big data capability, the paper divides the capability into three dimensions: big data system capability, big data human capability and big data management capability. The main conclusions are as follows: (1) Big data management capability has a significant positive impact on big data human capability and big data system capability. (2) Big data system capability has a significant positive impact on big data human capability. (3) Big data system capability and big data management capability have a significant positive effect on smart service performance. (4) The impact of big data human capability on smart service performance is not however significant enough to bring about the improvements which the government seeks.",IEEE Access,02 February 2021,50523 - 50537,2169-3536,,10.1109/ACCESS.2021.3056486,https://doi.org/10.1109/ACCESS.2021.3056486,IEEE
Big Data Meet Green Challenges: Greening Big Data,Jinsong Wu; Song Guo; Jie Li; Deze Zeng; ; ; ; ,https://ieeexplore.ieee.org/document/7473821/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7473821,"Nowadays, there are two significant tendencies, how to process the enormous amount of data, big data, and how to deal with the green issues related to sustainability and environmental concerns. An interesting question is whether there are inherent correlations between the two tendencies in general. To answer this question, this paper firstly makes a comprehensive literature survey on how to green big data systems in terms of the whole life cycle of big data processing, and then this paper studies the relevance between big data and green metrics and proposes two new metrics, effective energy efficiency and effective resource efficiency in order to bring new views and potentials of green metrics for the future times of big data.",IEEE Systems Journal,19 May 2016,873 - 887,,,10.1109/JSYST.2016.2550538,https://doi.org/10.1109/JSYST.2016.2550538,IEEE
Curriculum Reform in Big Data Education at Applied Technical Colleges and Universities in China,Xin Li; Xiaoping Fan; Xilong Qu; Guang Sun; Chen Yang; Biao Zuo; Zhifang Liao; ; ; ; ; ; ; ,https://ieeexplore.ieee.org/document/8822937/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8822937,"With the boom in data science, big data education has received increasing attention from all kinds of colleges and universities in China, and many of them are in a rush to offer big data education. This paper first analyzes the major areas of big data capability training and the Chinese market needs for various kinds of data science talent. Then, it discusses the curriculum design process for the “Data Science & Big Data Technology” bachelor's degree program, and summarizes some detailed approaches to improving teaching experiments. Finally, this paper proposes a graduating student profile for big data education at applied technical colleges and universities in China. The authors' main ideas include that, at the applied technical colleges and universities, a) a suitable graduating student orientation should be determined as the big data talent needs are hierarchical; b) the redesigned curriculum in big data education should provide students more practical capabilities and knowledge; c) the teaching of the existing mainstream big data technologies and tools should be significant components in the syllabi of big data education.",IEEE Access,03 September 2019,125511 - 125521,2169-3536,,10.1109/ACCESS.2019.2939196,https://doi.org/10.1109/ACCESS.2019.2939196,IEEE
Chronic Diseases and Health Monitoring Big Data: A Survey,Rongheng Lin; Zezhou Ye; Hao Wang; Budan Wu; ; ; ; ,https://ieeexplore.ieee.org/document/8345663/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8345663,"With the advancement of technology in data science and network technology, the world has stepped into the Era of Big Data, and the medical field is rich in data suitable for analysis. Thus, in recent years, there has been much research in medical big data, mainly targeting data collection, data analysis, and visualization. However, very few works provide a full survey of the medical big data on chronic diseases and health monitoring. This review investigates recent research efforts and conducts a comprehensive overview of the work on medical big data, especially as related to chronic diseases and health monitoring. It focuses on the full cycles of the big data processing, which includes medical big data preprocessing, big data tools and algorithms, big data visualization, and security issues in big data. It also attempts to combine common big data technologies with special medical needs by analyzing in detail existing works of medical big data. To the best of our knowledge, this is the first survey that targets chronic diseases and health monitoring big data technologies.",IEEE Reviews in Biomedical Engineering,24 April 2018,275 - 288,,29993699,10.1109/RBME.2018.2829704,https://doi.org/10.1109/RBME.2018.2829704,IEEE
Real-Time Big Data Analytical Architecture for Remote Sensing Application,Muhammad Mazhar Ullah Rathore; Anand Paul; Awais Ahmad; Bo-Wei Chen; Bormin Huang; Wen Ji; ; ; ; ; ; ,https://ieeexplore.ieee.org/document/7109130/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7109130,"The assets of remote senses digital world daily generate massive volume of real-time data (mainly referred to the term “Big Data”), where insight information has a potential significance if collected and aggregated effectively. In today's era, there is a great deal added to real-time remote sensing Big Data than it seems at first, and extracting the useful information in an efficient manner leads a system toward a major computational challenges, such as to analyze, aggregate, and store, where data are remotely collected. Keeping in view the above mentioned factors, there is a need for designing a system architecture that welcomes both real-time, as well as offline data processing. Therefore, in this paper, we propose real-time Big Data analytical architecture for remote sensing satellite application. The proposed architecture comprises three main units, such as 1) remote sensing Big Data acquisition unit (RSDU); 2) data processing unit (DPU); and 3) data analysis decision unit (DADU). First, RSDU acquires data from the satellite and sends this data to the Base Station, where initial processing takes place. Second, DPU plays a vital role in architecture for efficient processing of real-time Big Data by providing filtration, load balancing, and parallel processing. Third, DADU is the upper layer unit of the proposed architecture, which is responsible for compilation, storage of the results, and generation of decision based on the results received from DPU. The proposed architecture has the capability of dividing, load balancing, and parallel processing of only useful data. Thus, it results in efficiently analyzing real-time remote sensing Big Data using earth observatory system. Furthermore, the proposed architecture has the capability of storing incoming raw data to perform offline analysis on largely stored dumps, when required. Finally, a detailed analysis of remotely sensed earth observatory Big Data for land and sea area are provided using Hadoop. In addition, va...",IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,15 May 2015,4610 - 4621,,,10.1109/JSTARS.2015.2424683,https://doi.org/10.1109/JSTARS.2015.2424683,IEEE
"Harnessing Big Data Analytics for Healthcare: A Comprehensive Review of Frameworks, Implications, Applications, and Impacts",Awais Ahmed; Rui Xi; Mengshu Hou; Syed Attique Shah; Sufian Hameed; ; ; ; ; ,https://ieeexplore.ieee.org/document/10274958/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10274958,"Big Data Analytics (BDA) has garnered significant attention in both academia and industries, particularly in sectors such as healthcare, owing to the exponential growth of data and advancements in technology. The integration of data from diverse sources and the utilization of advanced analytical techniques has the potential to revolutionize healthcare by improving diagnostic accuracy, enabling personalized medicine, and enhancing patient outcomes. In this paper, we aim to provide a comprehensive literature review on the application of big data analytics in healthcare, focusing on its ecosystem, applications, and data sources. To achieve this, an extensive analysis of scientific studies published between 2013 and 2023 was conducted and overall 180 scientific studies were thoroughly evaluated, establishing a strong foundation for future research and identifying collaboration opportunities in the healthcare domain. The study delves into various application areas of BDA in healthcare, highlights successful implementations, and explores their potential to enhance healthcare outcomes while reducing costs. Additionally, it outlines the challenges and limitations associated with BDA in healthcare, discusses modelling tools and techniques, showcases deployed solutions, and presents the advantages of BDA through various real-world use cases. Furthermore, this study identifies and discusses key open research challenges in the field of big data analytics in healthcare, aiming to push the boundaries and contribute to enhanced healthcare outcomes and decision-making processes.",IEEE Access,10 October 2023,112891 - 112928,2169-3536,,10.1109/ACCESS.2023.3323574,https://doi.org/10.1109/ACCESS.2023.3323574,IEEE
Protection of Big Data Privacy,Abid Mehmood; Iynkaran Natgunanathan; Yong Xiang; Guang Hua; Song Guo; ; ; ; ; ,https://ieeexplore.ieee.org/document/7460114/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7460114,"In recent years, big data have become a hot research topic. The increasing amount of big data also increases the chance of breaching the privacy of individuals. Since big data require high computational power and large storage, distributed systems are used. As multiple parties are involved in these systems, the risk of privacy violation is increased. There have been a number of privacy-preserving mechanisms developed for privacy protection at different stages (e.g., data generation, data storage, and data processing) of a big data life cycle. The goal of this paper is to provide a comprehensive overview of the privacy preservation mechanisms in big data and present the challenges for existing mechanisms. In particular, in this paper, we illustrate the infrastructure of big data and the state-of-the-art privacy-preserving mechanisms in each stage of the big data life cycle. Furthermore, we discuss the challenges and future research directions related to privacy preservation in big data.",IEEE Access,27 April 2016,1821 - 1834,2169-3536,,10.1109/ACCESS.2016.2558446,https://doi.org/10.1109/ACCESS.2016.2558446,IEEE
Big Data Meet Green Challenges: Big Data Toward Green Applications,Jinsong Wu; Song Guo; Jie Li; Deze Zeng; ; ; ; ,https://ieeexplore.ieee.org/document/7473815/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7473815,"Big data are widely recognized as being one of the most powerful drivers to promote productivity, improve efficiency, and support innovation. It is highly expected to explore the power of big data and turn big data into big values. To answer the interesting question whether there are inherent correlations between the two tendencies of big data and green challenges, a recent study has investigated the issues on greening the whole life cycle of big data systems. This paper would like to discover the relations between the trend of big data era and that of the new generation green revolution through a comprehensive and panoramic literature survey in big data technologies toward various green objectives and a discussion on relevant challenges and future directions.",IEEE Systems Journal,19 May 2016,888 - 900,,,10.1109/JSYST.2016.2550530,https://doi.org/10.1109/JSYST.2016.2550530,IEEE
Leveraging Big Data Analytics for Enhanced Clinical Decision-Making in Healthcare,Fatima Hussain; Muhammad Nauman; Abdullah Alghuried; Adi Alhudhaif; Nadeem Akhtar; ; ; ; ; ,https://ieeexplore.ieee.org/document/10314483/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10314483,"Recently, the rate of data generation has reached unprecedented levels, leading to a huge amount of data volume. In addition, modern-day computing systems generate data in diverse formats, ranging from unstructured to structured and semi-structured. As technological advancements experience exponential growth, novel trends, and strategies are emerging in the field of Big Data to enhance data quality and derive valuable insights, particularly in industries like healthcare. The primary objective of this study is to investigate the challenges and applications of Big Data in healthcare, with a specific focus on improving clinical decision-making. By analyzing 185 papers published between 2012 and 2023, this review article aims to provide a comprehensive overview of the techniques and methods employed in utilizing Big Data Analytics in the healthcare domain. Furthermore, the article aspires to assist the research community in identifying suitable approaches and methodologies for their healthcare-related studies.",IEEE Access,09 November 2023,127817 - 127836,2169-3536,,10.1109/ACCESS.2023.3332030,https://doi.org/10.1109/ACCESS.2023.3332030,IEEE
"A Comprehensive Analysis of Healthcare Big Data Management, Analytics and Scientific Programming",Shah Nazir; Sulaiman Khan; Habib Ullah Khan; Shaukat Ali; Iván García-Magariño; Rodziah Binti Atan; Muhammad Nawaz; ; ; ; ; ; ; ,https://ieeexplore.ieee.org/document/9096305/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9096305,"Healthcare systems are transformed digitally with the help of medical technology, information systems, electronic medical records, wearable and smart devices, and handheld devices. The advancement in the medical big data, along with the availability of new computational models in the field of healthcare, has enabled the caretakers and researchers to extract relevant information and visualize the healthcare big data in a new spectrum. The role of medical big data becomes a challenging task in the form of storage, required information retrieval within a limited time, cost efficient solutions in terms care, and many others. Early decision making based healthcare system has massive potential for dropping the cost of care, refining quality of care, and reducing waste and error. Scientific programming play a significant role to overcome the existing issues and future problems involved in the management of large scale data in healthcare, such as by assisting in the processing of huge data volumes, complex system modelling, and sourcing derivations from healthcare data and simulations. Therefore, to address this problem efficiently a detailed study and analysis of the available literature work is required to facilitate the doctors and practitioners for making the decisions in identifying the disease and suggest treatment accordingly. The peer reviewed reputed journals are selected for the accumulated of published research work during the period ranges from 2015 - 2019 (a portion of 2020 is also included). A total of 127 relevant articles (conference papers, journal papers, book section, and survey papers) are selected for the assessment and analysis purposes. The proposed research work organizes and summarizes the existing published research work based on the research questions defined and keywords identified for the search process. This analysis on the existence research work will help the doctors and practitioners to make more authentic decisions, which ultimately will he...",IEEE Access,19 May 2020,95714 - 95733,2169-3536,,10.1109/ACCESS.2020.2995572,https://doi.org/10.1109/ACCESS.2020.2995572,IEEE
Data mining with big data,Xindong Wu; Xingquan Zhu; Gong-Qing Wu; Wei Ding; ; ; ; ,https://ieeexplore.ieee.org/document/6547630/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6547630,"Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.",IEEE Transactions on Knowledge and Data Engineering,26 June 2013,97 - 107,,,10.1109/TKDE.2013.109,https://doi.org/10.1109/TKDE.2013.109,IEEE
A Framework for Big Data Governance to Advance RHINs: A Case Study of China,Quan Li; Lan Lan; Nianyin Zeng; Lei You; Jin Yin; Xiaobo Zhou; Qun Meng; ; ; ; ; ; ; ,https://ieeexplore.ieee.org/document/8689011/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8689011,"The emergence of big data presents a serious challenge to the fast growth of regional health information networks (RHINs) globally. In China, many constructors of RHINs have spontaneously and independently created governance measures, which may be valuable as a point of reference for other countries. This paper aimed to propose a big data governance framework for healthcare data based on the governance activities associated with the processing of RHINs in China. Typical methodology for RHIN case studies in China, including rich personal experience in nationwide consulting, literature review, expert consultation, and interpretative structural modeling methods, was adopted. Based on the analysis of ten typical RHIN case studies, healthcare big data governance practices in China were summarized. A framework with 3 domains and 12 elements was proposed, which include a drive domain (big data strategy planning, laws and regulations, open transaction, and industry support), capability domain (healthcare big data organization, collection, storage, process and analysis, and usage), and support domain (healthcare big data resource planning, standards system, and privacy and security protection). We obtained 12 guidelines for healthcare big data governance. A big data governance framework with 3 domains and 12 elements was presented based on Chinese practice, which might serve as valuable references for the cross-dimensional development of RHINs, provide overall guidance for the sustainable development of regional health informatization, and contribute to realizing the business value of healthcare big data.",IEEE Access,12 April 2019,50330 - 50338,2169-3536,,10.1109/ACCESS.2019.2910838,https://doi.org/10.1109/ACCESS.2019.2910838,IEEE
A Comprehensive Review on Big Data for Industries: Challenges and Opportunities,Supriya Sarker; Mohammad Shamsul Arefin; Md Kowsher; Touhid Bhuiyan; Pranab Kumar Dhar; Oh-Jin Kwon; ; ; ; ; ; ,https://ieeexplore.ieee.org/document/9999445/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9999445,"Technological advancements in large industries like power, minerals, and manufacturing are generating massive data every second. Big data techniques have opened up numerous opportunities to utilize massive datasets in several effective ways to improve the efficacy of related industries. This paper presents a review of big data technologies used in the power, mineral, and manufacturing industries for various purposes. We analyze the meta-data of the collected papers before reviewing and selecting papers by applying selection criteria and paper quality assessment strategy. Then we propose a taxonomy of big data application areas in the power, mineral, and manufacturing industries. We have studied current big data architectures and techniques implemented in industry sectors and have uncovered the big data research gaps in industry sectors. To address the gaps, we point out some relevant research questions and, to answer the questions, we make some future research recommendations that might explore interesting research ideas for building a big data-driven industry. As the careful use of big data benefits every other industry sector; hence, supportive big data frameworks need to be developed to speed up the big data analysis process. Proper multi-dimensional big data assessment is also needed to take into account for serving effective data analysis tasks. Industry automation is also heavily influenced by the proper utilization of big data. While an intelligent agent can make many processes and heavy production loads in the manufacturing industry, it can work in a risky environment such as mines efficiently. To train agents for working in a specific environment big data can be used.",IEEE Access,26 December 2022,744 - 769,2169-3536,,10.1109/ACCESS.2022.3232526,https://doi.org/10.1109/ACCESS.2022.3232526,IEEE
"Big Data, Big Knowledge: Big Data for Personalized Healthcare",Marco Viceconti; Peter Hunter; Rod Hose; ; ; ,https://ieeexplore.ieee.org/document/7047725/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7047725,"The idea that the purely phenomenological knowledge that we can extract by analyzing large amounts of data can be useful in healthcare seems to contradict the desire of VPH researchers to build detailed mechanistic models for individual patients. But in practice no model is ever entirely phenomenological or entirely mechanistic. We propose in this position paper that big data analytics can be successfully combined with VPH technologies to produce robust and effective in silico medicine solutions. In order to do this, big data technologies must be further developed to cope with some specific requirements that emerge from this application. Such requirements are: working with sensitive data; analytics of complex and heterogeneous data spaces, including nontextual information; distributed data management under security and performance constraints; specialized analytics to integrate bioinformatics and systems biology information with clinical observations at tissue, organ and organisms scales; and specialized analytics to define the “physiological envelope” during the daily life of each patient. These domain-specific requirements suggest a need for targeted funding, in which big data technologies for in silico medicine becomes the research priority.",IEEE Journal of Biomedical and Health Informatics,24 February 2015,1209 - 1215,,26218867,10.1109/JBHI.2015.2406883,https://doi.org/10.1109/JBHI.2015.2406883,IEEE
A Comparative Perspective on Technologies of Big Data Value Chain,Ahmet Arif Aydin; ,https://ieeexplore.ieee.org/document/10281364/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10281364,"Data is one of the most valuable assets in the digital era because it may conceal hidden valuable insights. Diverse organizations in diverse domains overcome the challenges of the big data value chain by employing a wide range of technologies to meet their needs and achieve a variety of goals to support their decision-making. Due to the significance of data-oriented technologies, this paper presents a model of the big data value chain based on technologies used in the acquisition, storage, and analysis of data. The following are the paper’s contributions: First, a model of the big data value chain is developed to illustrate a comprehensive representation of the big data value chain that depicts the relationships between the characteristics of big data and the technologies associated with each category. Second, in contrast to previous research, this paper presents an overview of technologies for each category of the big data value chain. The third contribution of this paper is to assist researchers and developers of data-intensive systems in selecting the appropriate technology for their specific application development use cases by providing examples of applications and use cases from prominent papers in a variety of fields and by describing the capabilities and stages of the technologies being presented so that the right technology is used at the right time in the big data collection, processing, storage, and analytics tasks.",IEEE Access,11 October 2023,112133 - 112146,2169-3536,,10.1109/ACCESS.2023.3323160,https://doi.org/10.1109/ACCESS.2023.3323160,IEEE
A Survey on Big Data for Network Traffic Monitoring and Analysis,Alessandro D’Alconzo; Idilio Drago; Andrea Morichetta; Marco Mellia; Pedro Casas; ; ; ; ; ,https://ieeexplore.ieee.org/document/8789667/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8789667,"Network Traffic Monitoring and Analysis (NTMA) represents a key component for network management, especially to guarantee the correct operation of large-scale networks such as the Internet. As the complexity of Internet services and the volume of traffic continue to increase, it becomes difficult to design scalable NTMA applications. Applications such as traffic classification and policing require real-time and scalable approaches. Anomaly detection and security mechanisms require to quickly identify and react to unpredictable events while processing millions of heterogeneous events. At last, the system has to collect, store, and process massive sets of historical data for post-mortem analysis. Those are precisely the challenges faced by general big data approaches: Volume, Velocity, Variety, and Veracity. This survey brings together NTMA and big data. We catalog previous work on NTMA that adopt big data approaches to understand to what extent the potential of big data is being explored in NTMA. This survey mainly focuses on approaches and technologies to manage the big NTMA data, additionally briefly discussing big data analytics (e.g., machine learning) for the sake of NTMA. Finally, we provide guidelines for future work, discussing lessons learned, and research directions.",IEEE Transactions on Network and Service Management,06 August 2019,800 - 813,,,10.1109/TNSM.2019.2933358,https://doi.org/10.1109/TNSM.2019.2933358,IEEE
An Iterative Methodology for Defining Big Data Analytics Architectures,Roberto Tardío; Alejandro Maté; Juan Trujillo; ; ; ,https://ieeexplore.ieee.org/document/9264179/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9264179,"Thanks to the advances achieved in the last decade, the lack of adequate technologies to deal with Big Data characteristics such as Data Volume is no longer an issue. Instead, recent studies highlight that one of the main Big Data issues is the lack of expertise to select adequate technologies and build the correct Big Data architecture for the problem at hand. In order to tackle this problem, we present our methodology for the generation of Big Data pipelines based on several requirements derived from Big Data features that are critical for the selection of the most appropriate tools and techniques. Thus, thanks to our approach we reduce the required know-how to select and build Big Data architectures by providing a step-by-step methodology that leads Big Data architects into creating their Big Data Pipelines for the case at hand. Our methodology has been tested in two use cases.",IEEE Access,19 November 2020,210597 - 210616,2169-3536,,10.1109/ACCESS.2020.3039455,https://doi.org/10.1109/ACCESS.2020.3039455,IEEE
A Scalable Data Chunk Similarity Based Compression Approach for Efficient Big Sensing Data Processing on Cloud,Chi Yang; Jinjun Chen; ; ,https://ieeexplore.ieee.org/document/7412709/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7412709,"Big sensing data is prevalent in both industry and scientific research applications where the data is generated with high volume and velocity. Cloud computing provides a promising platform for big sensing data processing and storage as it provides a flexible stack of massive computing, storage, and software services in a scalable manner. Current big sensing data processing on Cloud have adopted some data compression techniques. However, due to the high volume and velocity of big sensing data, traditional data compression techniques lack sufficient efficiency and scalability for data processing. Based on specific on-Cloud data compression requirements, we propose a novel scalable data compression approach based on calculating similarity among the partitioned data chunks. Instead of compressing basic data units, the compression will be conducted over partitioned data chunks. To restore original data sets, some restoration functions and predictions will be designed. MapReduce is used for algorithm implementation to achieve extra scalability on Cloud. With real world meteorological big sensing data experiments on U-Cloud platform, we demonstrate that the proposed scalable compression approach based on data chunk similarity can significantly improve data compression efficiency with affordable data accuracy loss.",IEEE Transactions on Knowledge and Data Engineering,18 February 2016,1144 - 1157,,,10.1109/TKDE.2016.2531684,https://doi.org/10.1109/TKDE.2016.2531684,IEEE
Data-Enabled Digestive Medicine: A New Big Data Analytics Platform,Lu Yan; Weihong Huang; Liming Wang; Song Feng; Yonghong Peng; Jie Peng; ; ; ; ; ; ,https://ieeexplore.ieee.org/document/8892618/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8892618,"This paper presents a big data analystics platform for clinical research and practice in the Gastroenterology Department of Xiangya Hospital at Central South University in China. This platform features a comprehensive and systematic support of big data in digestive medicine including geneneral health management, clinical gastroenterology practice, and related genomics research, which is proven to be helpful in real world clinical practices. A typical use case of integrated analysis based on electronic medical records and colonoscopy data was presented and discussed, the analaystic report on risk factors of colorectal diseases shows a reasonable recommendation about the age when people should start to screen the colorectal cancer, which could be very useful to individual and group health management for the general population in China.",IEEE/ACM Transactions on Computational Biology and Bioinformatics,06 November 2019,922 - 931,,31714231,10.1109/TCBB.2019.2951555,https://doi.org/10.1109/TCBB.2019.2951555,IEEE
ATCS: Auto-Tuning Configurations of Big Data Frameworks Based on Generative Adversarial Nets,Mingyu Li; Zhiqiang Liu; Xuanhua Shi; Hai Jin; ; ; ; ,https://ieeexplore.ieee.org/document/9031432/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9031432,"Big data processing frameworks (e.g., Spark, Storm) have been extensively used for massive data processing in the industry. To improve the performance and robustness of these frameworks, developers provide users with highly-configurable parameters. Due to the high-dimensional parameter space and complicated interactions of parameters, manual tuning of parameters is time-consuming and ineffective. Building performance-predicting models for big data frameworks is challenging for several reasons: (1) the significant time required to collect training data and (2) the poor accuracy of the prediction model when training data are limited. To meet this challenge, we proposes an auto-tuning configuration parameters system (ATCS), a new auto-tuning approach based on Generative Adversarial Nets (GAN). ATCS can build a performance prediction model with less training data and without sacrificing model accuracy. Moreover, an optimized Genetic Algorithm (GA) is used in ATCS to explore the parameter space for optimum solutions. To prove the effectiveness of ATCS, we select five frequently-used workloads in Spark, each of which runs on five different sized data sets. The results demonstrate that ATCS improves the performance of five frequently-used Spark workloads compared to the default configurations. We achieved a performance increase of 3.5× on average, with a maximum of 6.9×. To obtain similar model accuracy, experiment results also demonstrate that the quantity of ATCS training data is only 6% of Deep Neural Network (DNN) data, 13% of Support Vector Machine (SVM) data, 18% of Decision Tree (DT) data. Moreover, compared to other machine learning models, the average performance increase of ATCS is 1.7× that of DNN, 1.6× that of SVM, 1.7× that of DT on the five typical Spark programs.",IEEE Access,10 March 2020,50485 - 50496,2169-3536,,10.1109/ACCESS.2020.2979812,https://doi.org/10.1109/ACCESS.2020.2979812,IEEE
Social Set Analysis: A Set Theoretical Approach to Big Data Analytics,Ravi Vatrapu; Raghava Rao Mukkamala; Abid Hussain; Benjamin Flesch; ; ; ; ,https://ieeexplore.ieee.org/document/7462188/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7462188,"Current analytical approaches in computational social science can be characterized by four dominant paradigms: text analysis (information extraction and classification), social network analysis (graph theory), social complexity analysis (complex systems science), and social simulations (cellular automata and agent-based modeling). However, when it comes to organizational and societal units of analysis, there exists no approach to conceptualize, model, analyze, explain, and predict social media interactions as individuals' associations with ideas, values, identities, and so on. To address this limitation, based on the sociology of associations and the mathematics of set theory, this paper presents a new approach to big data analytics called social set analysis. Social set analysis consists of a generative framework for the philosophies of computational social science, theory of social data, conceptual and formal models of social data, and an analytical framework for combining big social data sets with organizational and societal data sets. Three empirical studies of big social data are presented to illustrate and demonstrate social set analysis in terms of fuzzy set-theoretical sentiment analysis, crisp set-theoretical interaction analysis, and event-studies-oriented set-theoretical visualizations. Implications for big data analytics, current limitations of the set-theoretical approach, and future directions are outlined.",IEEE Access,28 April 2016,2542 - 2571,2169-3536,,10.1109/ACCESS.2016.2559584,https://doi.org/10.1109/ACCESS.2016.2559584,IEEE
MRMondrian: Scalable Multidimensional Anonymisation for Big Data Privacy Preservation,Xuyun Zhang; Lianyong Qi; Wanchun Dou; Qiang He; Christopher Leckie; Ramamohanarao Kotagiri; Zoran Salcic; ; ; ; ; ; ; ,https://ieeexplore.ieee.org/document/8240631/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8240631,"Scalable data processing platforms built on cloud computing becomes increasingly attractive as infrastructure for supporting big data applications. But privacy concerns are one of the major obstacles to making use of public cloud platforms. Multidimensional anonymisation, a global-recoding generalisation scheme for privacy-preserving data publishing, has been a recent focus due to its capability of balancing data obfuscation and usability. Existing multidimensional anonymisation methods suffer from scalability problems when handling big data due to the impractical serial I/O cost. Given the recursive feature of multidimensional anonymisation, parallelisation is an ideal solution to scalability issues. However, it is still a challenge to use existing distributed and parallel paradigms directly for recursive computation. In this paper, we propose a scalable approach for big data multidimensional anonymisation based on MapReduce, a state-of-the-art data processing paradigm. Our basic idea is to partition a data set recursively into smaller partitions using MapReduce until all partitions can fit in the memory of a computing node. A tree indexing structure is proposed to achieve recursive computation. Moreover, we show the applicability of our approach to differential privacy. Experimental results on real-life data demonstrate that our approach can significantly improve the scalability of multidimensional anonymisation over existing methods.",IEEE Transactions on Big Data,27 December 2017,125 - 139,,,10.1109/TBDATA.2017.2787661,https://doi.org/10.1109/TBDATA.2017.2787661,IEEE
Critical Success Factors for Big Data: A Systematic Literature Review,Zaher Ali Al-Sai; Rosni Abdullah; Mohd Heikal Husin; ; ; ,https://ieeexplore.ieee.org/document/9127414/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9127414,"During the last few decades, many organizations have started recognizing the benefits of Big Data (BD) to drive their digital transformation and to gain faster insights from faster data. Making smart data-driven decisions will help the organizations to ride the waves toward invaluable investments. The successful implementation of Big Data projects depends on their alignment with the current organizational, technological, and analytical aspects. Identifying the Critical Success Factors (CSFs) for Big Data is fundamental to overcome the challenges surrounding Big Data Analytics (BDA) and implementation. In recent years, the investigations related to identifying the CSFs of Big Data and Big Data Analytics expanded on a large scale trying to address the limitations in existing publications and contribute to the body of knowledge. This paper aims to provide more understanding about the existing CSFs for Big Data Analytics and implementation and contributes to the body of knowledge by answering three research questions: 1) How many studies have investigated on Big Data CSFs for analytics and implementation?, 2) What are the existing CSFs for Big Data Analytics, and 3) What are the categories of Big Data Analytics CSFs?. By conducting a Systematic Literature Review (SLR) for the available studies related to Big Data CSFs in the last twelve years (2007-2019), a final list of sixteen (16) related articles was extracted and analyzed to identify the Big Data Analytics CSFs and their categories. Based on the descriptive qualitative content analysis method for the selected literature, this SLR paper identifies 74 CSFs for Big Data and proposes a classification schema and framework in terms of 5 categories, namely Organization, Technology, People, Data Management, and Governance. The findings of this paper could be used as a referential framework for a successful strategy and implementation of Big Data by formulating more effective data-driven decisions. Future work will investig...",IEEE Access,29 June 2020,118940 - 118956,2169-3536,,10.1109/ACCESS.2020.3005461,https://doi.org/10.1109/ACCESS.2020.3005461,IEEE
Identity-Based Dynamic Data Auditing for Big Data Storage,Tao Shang; Feng Zhang; Xingyue Chen; Jianwei Liu; Xinxi Lu; ; ; ; ; ,https://ieeexplore.ieee.org/document/8839836/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8839836,"Identity-based remote data auditing schemes can verify data integrity and provide a simple identity authentication and management for multiple users. However, prior works on identity-based remote data auditing lack the support of dynamic operations. In these schemes, tag generation is linked to the index of data block, which is related to update operations such as modification, insertion and deletion. If users perform dynamic operations on a data block, the tags of all subsequent blocks need to be modified. It means that if users want to update data on a big data platform, they have to download the whole file, update the file and send the updated file to the big data platform. Such pattern will bring huge communication overhead. In this paper, we propose an identity-based dynamic data auditing scheme which supports dynamic data operations, including modification, insertion and deletion. As far as we know, there is still no other identity-based data auditing scheme that supports dynamic operations. In particular, to achieve efficient dynamic operations, we use the data structure of Merkle hash tree for block tag authentication, which helps update data with integrity assurance. Analyses of security and performance show that the proposed scheme is efficient and secure.",IEEE Transactions on Big Data,16 September 2019,913 - 921,,,10.1109/TBDATA.2019.2941882,https://doi.org/10.1109/TBDATA.2019.2941882,IEEE
Approximate Clustering Ensemble Method for Big Data,Mohammad Sultan Mahmud; Joshua Zhexue Huang; Rukhsana Ruby; Alladoumbaye Ngueilbaye; Kaishun Wu; ; ; ; ; ,https://ieeexplore.ieee.org/document/10066202/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10066202,"Clustering a big distributed dataset of hundred gigabytes or more is a challenging task in distributed computing. A popular method to tackle this problem is to use a random sample of the big dataset to compute an approximate result as an estimation of the true result computed from the entire dataset. In this paper, instead of using a single random sample, we use multiple random samples to compute an ensemble result as the estimation of the true result of the big dataset. We propose a distributed computing framework to compute the ensemble result. In this framework, a big dataset is represented in the RSP data model as random sample data blocks managed in a distributed file system. To compute the ensemble clustering result, a set of RSP data blocks is randomly selected as random samples and clustered independently in parallel on the nodes of a cluster to generate the component clustering results. The component results are transferred to the master node, which computes the ensemble result. Since the random samples are disjoint and traditional consensus functions cannot be used, we propose two new methods to integrate the component clustering results into the final ensemble result. The first method uses component cluster centers to build a graph and the METIS algorithm to cut the graph into subgraphs, from which a set of candidate cluster centers is found. A hierarchical clustering method is then used to generate the final set of
k
cluster centers. The second method uses the clustering-by-passing-messages method to generate the final set of
k
cluster centers. Finally, the
k
-means algorithm was used to allocate the entire dataset into
k
clusters. Experiments were conducted on both synthetic and real-world datasets. The results show that the new ensemble clustering methods performed better than the comparison methods and that the distributed computing framework is efficient and scalable in clustering big datasets.",IEEE Transactions on Big Data,10 March 2023,1142 - 1155,,,10.1109/TBDATA.2023.3255003,https://doi.org/10.1109/TBDATA.2023.3255003,IEEE
Quality Anomaly Detection Using Predictive Techniques: An Extensive Big Data Quality Framework for Reliable Data Analysis,Elouataoui Widad; Elmendili Saida; Youssef Gahi; ; ; ,https://ieeexplore.ieee.org/document/10256169/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10256169,"The increasing reliance on Big Data analytics has highlighted the critical role of data quality in ensuring accurate and reliable results. Consequently, organizations aiming to leverage the power of Big Data recognize the crucial role of data quality as an integral component. One notable type of data quality anomaly observed in big datasets is the presence of outlier values. Detecting and addressing these outliers have become a subject of interest across diverse domains, leading to the development of numerous anomaly detection approaches. Although anomaly detection has witnessed a proliferation of practices in recent years, a significant gap remains in addressing anomalies related to the other aspects of data quality. Indeed, while most approaches focus on identifying anomalies that deviate from the expected patterns, they do not consider irregularities in data quality, such as missing, incorrect, or inconsistent data. Moreover, most of approaches are domain-correlated and lack the capability to detect anomalies in a generic manner. Thus, we aim through this paper to address this gap in the field and provide a holistic and effective solution for Big Data quality anomaly detection. To achieve this, we suggest a novel approach that allows a comprehensive detection of Big Data quality anomalies related to six quality dimensions: Accuracy, Consistency, Completeness, Conformity, Uniqueness, and Readability. Moreover, the framework allows for sophisticated detection of generic data quality anomalies through the implementation of an intelligent anomaly detection model without any correlation to a specific field. Furthermore, we introduce and measure a new metric called “Quality Anomaly Score,” which refers to the degree of anomalousness of the quality anomalies of each quality dimension and the entire dataset. Through the implementation and evaluation of our framework, the suggested framework has achieved an accuracy score of up to 99.91% and an F1-score of 98.07%.",IEEE Access,20 September 2023,103306 - 103318,2169-3536,,10.1109/ACCESS.2023.3317354,https://doi.org/10.1109/ACCESS.2023.3317354,IEEE
Toward Scalable Systems for Big Data Analytics: A Technology Tutorial,Han Hu; Yonggang Wen; Tat-Seng Chua; Xuelong Li; ; ; ; ,https://ieeexplore.ieee.org/document/6842585/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6842585,"Recent technological advancements have led to a deluge of data from distinctive domains (e.g., health care and scientific sensors, user-generated data, Internet and financial companies, and supply chain systems) over the past two decades. The term big data was coined to capture the meaning of this emerging trend. In addition to its sheer volume, big data also exhibits other unique characteristics as compared with traditional data. For instance, big data is commonly unstructured and require more real-time analysis. This development calls for new system architectures for data acquisition, transmission, storage, and large-scale data processing mechanisms. In this paper, we present a literature survey and system tutorial for big data analytics platforms, aiming to provide an overall picture for nonexpert readers and instill a do-it-yourself spirit for advanced audiences to customize their own big-data solutions. First, we present the definition of big data and discuss big data challenges. Next, we present a systematic framework to decompose big data systems into four sequential modules, namely data generation, data acquisition, data storage, and data analytics. These four modules form a big data value chain. Following that, we present a detailed survey of numerous approaches and mechanisms from research and industry communities. In addition, we present the prevalent Hadoop framework for addressing big data challenges. Finally, we outline several evaluation benchmarks and potential research directions for big data systems.",IEEE Access,24 June 2014,652 - 687,2169-3536,,10.1109/ACCESS.2014.2332453,https://doi.org/10.1109/ACCESS.2014.2332453,IEEE
Mobile Big Data: The Fuel for Data-Driven Wireless,Xiang Cheng; Luoyang Fang; Liuqing Yang; Shuguang Cui; ; ; ; ,https://ieeexplore.ieee.org/document/7945539/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7945539,"In the past decade, the smart phone evolution has accelerated the proliferation of the mobile Internet and spurred a new wave of mobile applications, leading to an unprecedented mobile data volume generated from the mobile devices, content servers, and network operators, which are mainly nonstructured. In this big data era, such nonstructured data fragments are pieced together such that, drastically differing from the traditional practice where services determine and define the data, data is becoming a proactive entity that may drive and even create new services. Compared with the so-termed 5V characteristics of generic big data, namely volume, variety, velocity, veracity, and value, mobile big data is distinct in its unique multidimensional, personalized, multisensory, and real-time features. In this survey, we provide in-depth and comprehensive coverage on the features, sources and applications of mobile big data, as well as the current state-of-the-art, challenges and opportunities for research and development in this field, with an emphasis on the user modeling, infrastructure supporting, data management, and knowledge discovery aspects.",IEEE Internet of Things Journal,09 June 2017,1489 - 1516,,,10.1109/JIOT.2017.2714189,https://doi.org/10.1109/JIOT.2017.2714189,IEEE
Big Data-Based Improved Data Acquisition and Storage System for Designing Industrial Data Platform,Daoqu Geng; Chengyun Zhang; Chengjing Xia; Xue Xia; Qilin Liu; Xinshuai Fu; ; ; ; ; ; ,https://ieeexplore.ieee.org/document/8681030/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8681030,"Big data-based acquisition and storage system (ASS) plays an important role in the design of industrial data platform. Many big data frameworks have been integrated compression and serialization method. These methods cannot meet the needs of industrial production information management for requiring time-consuming and mass storage. Based on the existing big data frameworks, we propose an enhanced industrial big data platform in order to reduce the data processing time while requiring fewer data storage space. Specifically, this paper focuses on evaluating the impact of multiple compression and serialization methods on big data platform performance and tries to choose optimal compression and serialization methods for the industrial data platform. Compared to the methods integrated in Hadoop and Spark, the experimental results showed the data compression time of the platform has been reduced by 73.9% with a less than 96% the size of data compressed, furthermore, the data serialization time has been reduced by 80.8%. With the increasing amount of data, it takes less time to compare with benchmark methods.",IEEE Access,03 April 2019,44574 - 44582,2169-3536,,10.1109/ACCESS.2019.2909060,https://doi.org/10.1109/ACCESS.2019.2909060,IEEE
A Survey of Big Data Analytics for Smart Forestry,Weitao Zou; Weipeng Jing; Guangsheng Chen; Yang Lu; Houbing Song; ; ; ; ; ,https://ieeexplore.ieee.org/document/8675907/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8675907,"Accurate and reliable forestry data can be obtained by means of continuous monitoring of forests using advanced technologies, which provides a major opportunity for the development of smart forestry. However, with the improvement of the precision and acquisition speed of data, the traditional data analysis, and storage technology cannot meet the performance requirements of current applications. Forestry big data has brought a new solution to the difficulties encountered in the course of forestry development, which refers to the application of big data technology to forestry data processing. In this paper, we summarize the research and work of the big data in smart forestry in recent years. First, we review the history of the emergence and development of forestry big data, and then briefly summarize the opportunities brought to the forestry by big data technology. One of the most important tasks of forestry big data is to organize the massive data reasonably and effectively and to calculate fast. Therefore, we propose a five-layer architecture model of forestry big data and summarize the related work of data storage, query, analysis, and application. Finally, the challenges of forestry big data are analyzed, and the trend of future development has prospected from three aspects.",IEEE Access,28 March 2019,46621 - 46636,2169-3536,,10.1109/ACCESS.2019.2907999,https://doi.org/10.1109/ACCESS.2019.2907999,IEEE
Resting-State fMRI Functional Connectivity: Big Data Preprocessing Pipelines and Topological Data Analysis,Angkoon Phinyomark; Esther Ibáñez-Marcelo; Giovanni Petri; ; ; ,https://ieeexplore.ieee.org/document/8052510/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8052510,"Resting state functional magnetic resonance imaging (rfMRI) can be used to measure functional connectivity and then identify brain networks and related brain disorders and diseases. To explore these complex networks, however, huge amounts of data are necessary. Recent advances in neuroimaging technologies, and the unique methodological approach of rfMRI, have enabled us to an era of Biomedical Big Data. The recent progress of big data sharing projects with their challenges are discussed. This increasing amount of neuroimaging data has greatly increased the importance of developing preprocessing pipelines and advanced analytic techniques, which are better at handling large-scale datasets. Before applying any analysis method on rfMRI data, several preprocessing steps need to be applied to reduce all unwanted effects. Three alternative ways to get access to big preprocessed rfMRI data are presented involving the minimal preprocessing pipelines. There are several commonly used methods to examine functional connectivity. However, they become limited in the analysis of big data, and a new tool to explore such data is necessary. We propose a number of novel methods rooted in algebraic topology and collectively referred to as Topological Data Analysis to rfMRI functional connectivity. Their properties for big data analysis are also discussed.",IEEE Transactions on Big Data,28 September 2017,415 - 428,,,10.1109/TBDATA.2017.2734883,https://doi.org/10.1109/TBDATA.2017.2734883,IEEE
An Integrated Methodology for Big Data Classification and Security for Improving Cloud Systems Data Mobility,Ismail Hababeh; Ammar Gharaibeh; Samer Nofal; Issa Khalil; ; ; ; ,https://ieeexplore.ieee.org/document/8594554/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594554,"The expand trend of cloud data mobility led to malicious data threats that necessitate using data protection techniques. Most cloud system applications contain valuable and confidential data, such as personal, trade, or health information. Threats on such data may put the cloud systems that hold these data at high risk. However, traditional security solutions are not capable of handling the security of big data mobility. The current security mechanisms are insufficient for big data due to their shortage of determining the data that should be protected or due to their intractable time complexity. Therefore, the demand for securing mobile big data has been increasing rapidly to avoid any potential risks. This paper proposes an integrated methodology to classify and secure big data before executing data mobility, duplication, and analysis. The necessity of securing big data mobility is determined by classifying the data according to the risk impact level of their contents into two categories; confidential and public. Based on the classification category, the impact of data security is studied and substantiated on the confidential data in the scope of Hadoop Distributed File System. It is revealed that the proposed approach can significantly improve the cloud systems data mobility.",IEEE Access,28 December 2018,9153 - 9163,2169-3536,,10.1109/ACCESS.2018.2890099,https://doi.org/10.1109/ACCESS.2018.2890099,IEEE
A survey of data partitioning and sampling methods to support big data analysis,,https://ieeexplore.ieee.org/document/9007871/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9007871,"Computer clusters with the shared-nothing architecture are the major computing platforms for big data processing and analysis. In cluster computing, data partitioning and sampling are two fundamental strategies to speed up the computation of big data and increase scalability. In this paper, we present a comprehensive survey of the methods and techniques of data partitioning and sampling with respect to big data processing and analysis. We start with an overview of the mainstream big data frameworks on Hadoop clusters. The basic methods of data partitioning are then discussed including three classical horizontal partitioning schemes: range, hash, and random partitioning. Data partitioning on Hadoop clusters is also discussed with a summary of new strategies for big data partitioning, including the new Random Sample Partition (RSP) distributed model. The classical methods of data sampling are then investigated, including simple random sampling, stratified sampling, and reservoir sampling. Two common methods of big data sampling on computing clusters are also discussed: record-level sampling and block-level sampling. Record-level sampling is not as efficient as block-level sampling on big distributed data. On the other hand, block-level sampling on data blocks generated with the classical data partitioning methods does not necessarily produce good representative samples for approximate computing of big data. In this survey, we also summarize the prevailing strategies and related work on sampling-based approximation on Hadoop clusters. We believe that data partitioning and sampling should be considered together to build approximate cluster computing frameworks that are reliable in both the computational and statistical respects.",Big Data Mining and Analytics,27 February 2020,85 - 101,,,10.26599/BDMA.2019.9020015,https://doi.org/10.26599/BDMA.2019.9020015,TUP
"Next-Generation Big Data Analytics: State of the Art, Challenges, and Future Research Topics",Zhihan Lv; Houbing Song; Pablo Basanta-Val; Anthony Steed; Minho Jo; ; ; ; ; ,https://ieeexplore.ieee.org/document/7866003/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7866003,"The term big data occurs more frequently now than ever before. A large number of fields and subjects, ranging from everyday life to traditional research fields (i.e., geography and transportation, biology and chemistry, medicine and rehabilitation), involve big data problems. The popularizing of various types of network has diversified types, issues, and solutions for big data more than ever before. In this paper, we review recent research in data types, storage models, privacy, data security, analysis methods, and applications related to network big data. Finally, we summarize the challenges and development of big data to predict current and future trends.",IEEE Transactions on Industrial Informatics,27 February 2017,1891 - 1899,,,10.1109/TII.2017.2650204,https://doi.org/10.1109/TII.2017.2650204,IEEE
Elevating Big Data Privacy: Innovative Strategies and Challenges in Data Abundance,Mohamed Elkawkagy; E. Elwan; Albandari Alsumayt; Heba Elbeh; Sumayh S. Aljameel; ; ; ; ; ,https://ieeexplore.ieee.org/document/10413360/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10413360,"The exponential growth of big data has ushered in transformative possibilities across various sectors, but it has also raised formidable privacy concerns. This article delves into the pressing need for enhancing big data privacy and explores innovative approaches to address this critical issue. In recent years, big data has been characterized by its immense volume, high velocity, and diverse data sources. These attributes have enabled organizations to gain unprecedented insights but have also exposed sensitive information to potential breaches. As such, ensuring the privacy of individuals and sensitive data within big data sets has emerged as a paramount concern. This article first elucidates the multifaceted nature of big data privacy, emphasizing its encompassment of privacy, confidentiality, integrity, and availability. It also acknowledges the challenges posed by existing privacy-preserving techniques, which often fall short of providing comprehensive protection for large and diverse data sets. The core focus of this article lies in presenting novel strategies and technologies designed to improve big data privacy. This article presents an innovative framework that combines advanced encryption methods, including fine-grained encryption techniques and differential privacy mechanisms specifically designed for the distinct traits of big data, like noisy techniques. To achieve this, the dataset undergoes categorization into key attributes, sensitive attributes, quasi attributes, and insensitive attributes. Subsequently, the fine-grained technique encrypts key and sensitive attributes, while the differential privacy mechanism encrypts the quasi attributes. To further substantiate the effectiveness of the proposed technique, this article references to empirical findings that demonstrate tangible improvements in big data privacy protection.",IEEE Access,24 January 2024,20931 - 20941,2169-3536,,10.1109/ACCESS.2024.3357943,https://doi.org/10.1109/ACCESS.2024.3357943,IEEE
"A Survey on Spatio-Temporal Big Data Analytics Ecosystem: Resource Management, Processing Platform, and Applications",Huanghuang Liang; Zheng Zhang; Chuang Hu; Yili Gong; Dazhao Cheng; ; ; ; ; ,https://ieeexplore.ieee.org/document/10356753/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10356753,"With the rapid evolution of the Internet, Internet of Things (IoT), and geographic information systems (GIS), spatio-temporal Big Data (STBD) is experiencing exponential growth, marking the onset of the STBD era. Recent studies have concentrated on developing algorithms and techniques for the collection, management, storage, processing, analysis, and visualization of STBD. Researchers have made significant advancements by enhancing STBD handling techniques, creating novel systems, and integrating spatio-temporal support into existing systems. However, these studies often neglect resource management and system optimization, crucial factors for enhancing the efficiency of STBD processing and applications. Additionally, the transition of STBD to the innovative Cloud-Edge-End unified computing system needs to be noticed. In this survey, we comprehensively explore the entire ecosystem of STBD analytics systems. We delineate the STBD analytics ecosystem and categorize the technologies used to process GIS data into five modules: STBD, computation resources, processing platform, resource management, and applications. Specifically, we subdivide STBD and its applications into geoscience-oriented and human-social activity-oriented. Within the processing platform module, we further categorize it into the data management layer (DBMS-GIS), data processing layer (BigData-GIS), data analysis layer (AI-GIS), and cloud native layer (Cloud-GIS). The resource management module and each layer in the processing platform are classified into three categories: task-oriented, resource-oriented, and cloud-based. Finally, we propose research agendas for potential future developments.",IEEE Transactions on Big Data,13 December 2023,174 - 193,,,10.1109/TBDATA.2023.3342619,https://doi.org/10.1109/TBDATA.2023.3342619,IEEE
Evaluating the Quality of Social Media Data in Big Data Architecture,Anne Immonen; Pekka Pääkkönen; Eila Ovaska; ; ; ,https://ieeexplore.ieee.org/document/7299603/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7299603,"The use of freely available online data is rapidly increasing, as companies have detected the possibilities and the value of these data in their businesses. In particular, data from social media are seen as interesting as they can, when properly treated, assist in achieving customer insight into business decision making. However, the unstructured and uncertain nature of this kind of big data presents a new kind of challenge: how to evaluate the quality of data and manage the value of data within a big data architecture? This paper contributes to addressing this challenge by introducing a new architectural solution to evaluate and manage the quality of social media data in each processing phase of the big data pipeline. The proposed solution improves business decision making by providing real-time, validated data for the user. The solution is validated with an industrial case example, in which the customer insight is extracted from social media data in order to determine the customer satisfaction regarding the quality of a product.",IEEE Access,16 October 2015,2028 - 2043,2169-3536,,10.1109/ACCESS.2015.2490723,https://doi.org/10.1109/ACCESS.2015.2490723,IEEE
QuantCloud: Enabling Big Data Complex Event Processing for Quantitative Finance Through a Data-Driven Execution,Peng Zhang; Xiang Shi; Samee U. Khan; ; ; ,https://ieeexplore.ieee.org/document/8386682/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8386682,"Quantitative Finance (QF) utilizes increasingly sophisticated mathematic models and advanced computer techniques to predict the movement of global markets, and price the derivatives and other assets. Being able to react quickly and intelligently to fast-changing markets is a decisive success factor for trading companies. To date, the rise of QF requires an integrated toolchain of enabling technologies to carry out complex event processing on the explosive growth and diversified forms of market metadata, in pursuit of a microsecond latency on an Exabyte-level dataset. Inspired by this, we present a data-driven execution paradigm that untangles the dependencies of complex processing events and integrate the paradigm with a big data infrastructure that streams time series data. This integrated platform is termed as the QuantCloud platform. Essentially, QuantCloud executes the complex event processing in a data-driven mode and manages large amounts of diversified market data in a data-parallel mode. To show its practicability and performance, we develop a prototype and benchmark by applying real-world QF research models on the New York Stock Exchange (NYSE) data. Using this prototype, we demonstrate this platform with an application to: (i) data cleaning and aggregating (including the computing of logarithmic returns from tick data and the finding the medians of grouped data) and (ii) data modeling: the autoregressive-moving average (ARMA) model. The performance results show that (a) this platform obtains a high throughput (usually in the order of millions of tick messages per second) and a sub-microsecond latency; (b) it fully executes data-dependent tasks through a data-driven execution; and (c) it implements a modular design approach for rapidly developing these data-crunching methods and QF research models. This platform resulting from an aggregated effort of the data-driven execution and big data infrastructure, offers the financial engineers with new insights and ...",IEEE Transactions on Big Data,15 June 2018,564 - 575,,,10.1109/TBDATA.2018.2847629,https://doi.org/10.1109/TBDATA.2018.2847629,IEEE
Big Data Challenges and Data Aggregation Strategies in Wireless Sensor Networks,Sabrina Boubiche; Djallel Eddine Boubiche; Azeddine Bilami; Homero Toral-Cruz; ; ; ; ,https://ieeexplore.ieee.org/document/8353765/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8353765,"The emergence of new data handling technologies and analytics enabled the organization of big data in processes as an innovative aspect in wireless sensor networks (WSNs). Big data paradigm, combined with WSN technology, involves new challenges that are necessary to resolve in parallel. Data aggregation is a rapidly emerging research area. It represents one of the processing challenges of big sensor networks. This paper introduces the big data paradigm, its main dimensions that represent one of the most challenging concepts, and its principle analytic tools which are more and more introduced in the WSNs technology. The paper also presents the big data challenges that must be overcome to efficiently manipulate the voluminous data, and proposes a new classification of these challenges based on the necessities and the challenges of WSNs. As the big data aggregation challenge represents the center of our interest, this paper surveys its proposed strategies in WSNs.",IEEE Access,03 May 2018,20558 - 20571,2169-3536,,10.1109/ACCESS.2018.2821445,https://doi.org/10.1109/ACCESS.2018.2821445,IEEE
Big Data Platform for Educational Analytics,Amr A. Munshi; Ahmad Alhindi; ; ,https://ieeexplore.ieee.org/document/9393907/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9393907,"Huge amounts of educational data are being produced, and a common challenge that many educational organizations confront, is finding an effective method to harness and analyze this data for continuously delivering enhanced education. Nowadays, the educational data is evolving and has become large in volume, wide in variety and high in velocity. This produced data needs to be handled in an efficient manner to extract value and make informed decisions. For that, this paper confronts such data as a big data challenge and presents a comprehensive platform tailored to perform educational big data analytical applications. Further, present an effective environment for non-data scientists and people in the educational sector to apply their demanding educational big data applications. The implementation stages of the educational big data platform on a cloud computing platform and the organization of educational data in a data lake architecture are highlighted. Furthermore, two analytical applications are performed to test the feasibility of the presented platform in discovering knowledge that potentially promotes the educational institutions.",IEEE Access,02 April 2021,52883 - 52890,2169-3536,,10.1109/ACCESS.2021.3070737,https://doi.org/10.1109/ACCESS.2021.3070737,IEEE
A Scalable Multi-Data Sources Based Recursive Approximation Approach for Fast Error Recovery in Big Sensing Data on Cloud,Chi Yang; Xianghua Xu; Kotagiri Ramamohanarao; Jinjun Chen; ; ; ; ,https://ieeexplore.ieee.org/document/8627988/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8627988,"Big sensing data is commonly encountered from various surveillance or sensing systems. Sampling and transferring errors are commonly encountered during each stage of sensing data processing. How to recover from these errors with accuracy and efficiency is quite challenging because of high sensing data volume and unrepeatable wireless communication environment. While Cloud provides a promising platform for processing big sensing data, however scalable and accurate error recovery solutions are still need. In this paper, we propose a novel approach to achieve fast error recovery in a scalable manner on cloud. This approach is based on the prediction of a recovery replacement data by making multiple data sources based approximation. The approximation process will use coverage information carried by data units to limit the algorithm in a small cluster of sensing data instead of a whole data spectrum. Specifically, in each sensing data cluster, a Euclidean distance based approximation is proposed to calculate a time series prediction. With the calculated time series, a detected error can be recovered with a predicted data value. Through the experiment with real world meteorological data sets on cloud, we demonstrate that the proposed error recovery approach can achieve high accuracy in data approximation to replace the original data error. At the same time, with MapReduce based implementation for scalability, the experimental results also show significant efficiency on time saving.",IEEE Transactions on Knowledge and Data Engineering,28 January 2019,841 - 854,,,10.1109/TKDE.2019.2895612,https://doi.org/10.1109/TKDE.2019.2895612,IEEE
Soft and Declarative Fishing of Information in Big Data Lake,Bożena Małysiak-Mrozek; Marek Stabla; Dariusz Mrozek; ; ; ,https://ieeexplore.ieee.org/document/8314734/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8314734,"In recent years, many fields that experience a sudden proliferation of data, which increases the volume of data that must be processed and the variety of formats the data is stored in have been identified. This causes pressure on existing compute infrastructures and data analysis methods, as more and more data are considered as a useful source of information for making critical decisions in particular fields. Among these fields exist several areas related to human life, e.g., various branches of medicine, where the uncertainty of data complicates the data analysis, and where the inclusion of fuzzy expert knowledge in data processing brings many advantages. In this paper, we show how fuzzy techniques can be incorporated in big data analytics carried out with the declarative U-SQL language over a big data lake located on the cloud. We define the concept of big data lake together with the Extract, Process, and Store process performed while schematizing and processing data from the Data Lake, and while storing results of the processing. Our solution, developed as a Fuzzy Search Library for Data Lake, introduces the possibility of massively parallel, declarative querying of big data lake with simple and complex fuzzy search criteria, using fuzzy linguistic terms in various data transformations, and fuzzy grouping. Presented ideas are exemplified by a distributed analysis of large volumes of biomedical data on Microsoft Azure cloud. Results of performed tests confirm that the presented solution is highly scalable on the Cloud and is a successful step toward soft and declarative processing of data on a large scale. The solution presented in this paper directly addresses three characteristics of big data, i.e., volume, variety, and velocity, and indirectly addresses, veracity and value.",IEEE Transactions on Fuzzy Systems,12 March 2018,2732 - 2747,,,10.1109/TFUZZ.2018.2812157,https://doi.org/10.1109/TFUZZ.2018.2812157,IEEE
Cost Minimization for Big Data Processing in Geo-Distributed Data Centers,Lin Gu; Deze Zeng; Peng Li; Song Guo; ; ; ; ,https://ieeexplore.ieee.org/document/6762920/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6762920,"The explosive growth of demands on big data processing imposes a heavy burden on computation, storage, and communication in data centers, which hence incurs considerable operational expenditure to data center providers. Therefore, cost minimization has become an emergent issue for the upcoming big data era. Different from conventional cloud services, one of the main features of big data services is the tight coupling between data and computation as computation tasks can be conducted only when the corresponding data are available. As a result, three factors, i.e., task assignment, data placement, and data movement, deeply influence the operational expenditure of data centers. In this paper, we are motivated to study the cost minimization problem via a joint optimization of these three factors for big data services in geo-distributed data centers. To describe the task completion time with the consideration of both data transmission and computation, we propose a 2-D Markov chain and derive the average task completion time in closed-form. Furthermore, we model the problem as a mixed-integer nonlinear programming and propose an efficient solution to linearize it. The high efficiency of our proposal is validated by extensive simulation-based studies.",IEEE Transactions on Emerging Topics in Computing,11 March 2014,314 - 323,,,10.1109/TETC.2014.2310456,https://doi.org/10.1109/TETC.2014.2310456,IEEE
Artificial Intelligence in Finance: Coffee Commodity Trading Big Data for Informed Decision Making,Ngoc-Bao-van Le; Yeong-Seok Seo; Jun-Ho Huh; ; ; ,https://ieeexplore.ieee.org/document/10549867/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10549867,"Coffee, the second-largest global soft commodity, can take advantage of a comprehensive mining of daily and historical market data for more effective informed trading decisions. Advanced ICT and data mining technologies can change the trading market operation. The existing systems are confronted with certain constraints, including incomplete data, insufficient documentation for storage, and a requirement for a scalable infrastructure for big data analytics, such as a data warehouse or data lakehouse. To address this issue, the paper presents a design and implementation of a coffee commodity trading big data warehouse capable of analyzing various essential parameters for supporting informed decision-making. First, the designed system can automatically collect coffee trading data for New York Arabica coffee futures prices from selected worldwide reports and financial data portals. Next, the Extract, transform, and load (ETL) process is adopted to ingest coffee futures trading crawled data into the 3 layers data warehouse. Finally, the analytical system will extract and visualize selected key dimensions that influence coffee futures prices within different observation windows and perspectives. As a result, we implement a prototype of a coffee trading data warehouse on the crawled data from January 2000 to October 2022 and visualize trends in coffee futures prices based on the collected data for informed decision-making. The construction system is capable of stably operating and processing large volumes of transaction data. This paper will be valuable documentation for reference and decision support for coffee commodity trading enterprises and contribute to the development of future forecasting algorithms.",IEEE Access,05 June 2024,91780 - 91792,2169-3536,,10.1109/ACCESS.2024.3409762,https://doi.org/10.1109/ACCESS.2024.3409762,IEEE
A Secure High-Order Lanczos-Based Orthogonal Tensor SVD for Big Data Reduction in Cloud Environment,Jun Feng; Laurence T. Yang; Guohui Dai; Wei Wang; Deqing Zou; ; ; ; ; ,https://ieeexplore.ieee.org/document/8283808/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8283808,"Singular value decomposition (SVD) has been applied to cyber security and cyber forensics since it can reduce data. However, SVD is hard to reduce high-order big data because it is designed for only matrix data initially. Reducing high-order big data is desired for cyber security applications, and is a very challenging issue. In this paper, we propose a novel orthogonal tensor SVD method using big data techniques for high-order big data (naturally represented as tensors) reduction, which can be extensively used in big data applications of cyber security and cyber forensics. More specifically, we first present a high-order lanczos-based orthogonal tensor SVD algorithm to reduce high-order data. Then, for utilizing the incomparable benefits of cloud, we develop a secure orthogonal tensor SVD method to outsource the computation task of the orthogonal tensor SVD algorithm to cloud. The secure orthogonal tensor SVD method can protect data security from untrusted cloud by applying garbled circuits to the orthogonal tensor SVD algorithm. This is, to our best knowledge, the first work to address high-order big data reduction by employing cloud computing. Finally, we analyze the security and efficiency of our proposed orthogonal tensor SVD on synthetic dataset and real network intrusion detection dataset, and the results demonstrate that our proposed method is very promising for big data reduction.",IEEE Transactions on Big Data,08 February 2018,355 - 367,,,10.1109/TBDATA.2018.2803841,https://doi.org/10.1109/TBDATA.2018.2803841,IEEE
SEEN: A Selective Encryption Method to Ensure Confidentiality for Big Sensing Data Streams,Deepak Puthal; Xindong Wu; Nepal Surya; Rajiv Ranjan; Jinjun Chen; ; ; ; ; ,https://ieeexplore.ieee.org/document/7921576/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7921576,"Resource constrained sensing devices are being used widely to build and deploy self-organizing wireless sensor networks for a variety of critical applications such as smart cities, smart health, precision agriculture and industrial control systems. Many such devices sense the deployed environment and generate a variety of data and send them to the server for analysis as data streams. A Data Stream Manager (DSM) at the server collects the data streams (often called big data) to perform real time analysis and decision-making for these critical applications. A malicious adversary may access or tamper with the data in transit. One of the challenging tasks in such applications is to assure the trustworthiness of the collected data so that any decisions are made on the processing of correct data. Assuring high data trustworthiness requires that the system satisfies two key security properties: confidentiality and integrity. To ensure the confidentiality of collected data, we need to prevent sensitive information from reaching the wrong people by ensuring that the right people are getting it. Sensed data are always associated with different sensitivity levels based on the sensitivity of emerging applications or the sensed data types or the sensing devices. For example, a temperature in a precision agriculture application may not be as sensitive as monitored data in smart health. Providing multilevel data confidentiality along with data integrity for big sensing data streams in the context of near real time analytics is a challenging problem. In this paper, we propose a Selective Encryption (SEEN) method to secure big sensing data streams that satisfies the desired multiple levels of confidentiality and data integrity. Our method is based on two key concepts: common shared keys that are initialized and updated by DSM without requiring retransmission, and a seamless key refreshment process without interrupting the data stream encryption/decryption. Theoretical analyses and e...",IEEE Transactions on Big Data,08 May 2017,379 - 392,,,10.1109/TBDATA.2017.2702172,https://doi.org/10.1109/TBDATA.2017.2702172,IEEE
An Improved Secure High-Order-Lanczos Based Orthogonal Tensor SVD for Outsourced Cyber-Physical-Social Big Data Reduction,Jun Feng; Laurence T. Yang; Guohui Dai; Jinjun Chen; Zheng Yan; ; ; ; ; ,https://ieeexplore.ieee.org/document/8536475/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8536475,"Cyber-physical-social big data concern heterogeneous, multiaspect, large-volume data generated in cyber-physical-social systems (CPSS). Orthogonal tensor SVD (OTSVD) has emerged as a powerful tool to reduce cyber-physical-social big data. In this work, we propose an improved secure high-order-Lanczos based OTSVD for cyber-physical-social big data reduction in clouds. Specifically, to take advantage of the parallel processing capability of cloud computing, the improved secure high-order Lanczos algorithm is derived by restructuring the original high-order Lanczos algorithm such that only one synchronization point per iteration is required. To protect data privacy, the improved secure high-order-Lanczos based OTSVD employs homomorphic encryption integrated with batching technique, and garbled circuits, and makes all computations of the OTSVD algorithm in clouds come true. To our knowledge, this is the first study to efficiently tackle big data reduction in clouds in a privacy-preserving manner. Finally, we prove that our improved approach is secure in semi-trusted model. And we evaluate the proposed improved secure OTSVD on real datasets. The results show that our proposed improved secure approach is efficient and scalable for cyber-physical-social big data reduction.",IEEE Transactions on Big Data,15 November 2018,808 - 818,,,10.1109/TBDATA.2018.2881441,https://doi.org/10.1109/TBDATA.2018.2881441,IEEE
A New Methodology for Storing Consistent Fuzzy Geospatial Data in Big Data Environment,Besma Khalfi; Cyril de Runz; Sami Faiz; Herman Akdag; ; ; ; ,https://ieeexplore.ieee.org/document/7974765/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7974765,"In this era of big data, as relational databases are inefficient, NoSQL databases are a workable solution for data storage. In this context, one of the key issues is the veracity and therefore the data quality. Indeed, as with classic data, geospatial big data are generally fuzzy even though they are stored as crisp data (perfect data). Hence, if data are geospatial and fuzzy, additional complexities appear because of the complex syntax and semantic features of such data. The NoSQL databases do not offer strict data consistency. Therefore, new challenges are needed to be overcome to develop efficient methods that simultaneously ensure the performance and the consistency in storing fuzzy geospatial big data. This paper presents a new methodology that tackles the storage issues and validates the fuzzy spatial entities' consistency in a document-based NoSQL system. Consequently, first, to better express the structure of fuzzy geospatial data in such a system, we present a logical model called Fuzzy GeoJSON schema. Second, for consistent storage, we implement a schema-driven pipeline based on the Fuzzy GeoJSON schema and semantic constraints.",IEEE Transactions on Big Data,11 July 2017,468 - 482,,,10.1109/TBDATA.2017.2725904,https://doi.org/10.1109/TBDATA.2017.2725904,IEEE
BIG DATA for Healthcare: A Survey,Safa Bahri; Nesrine Zoghlami; Mourad Abed; João Manuel R. S. Tavares; ; ; ; ,https://ieeexplore.ieee.org/document/8585021/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8585021,"Recently, the massification of new technologies, which has been adopted by a large majority of the world population, has accumulated a tremendous amount of data, including clinical data. This clinical data have been gathered up and interpreted by medical organizations in order to gain insights and knowledge useful for clinical decisions, drug recommendations, and better diagnoses, among many other uses. This paper highlights the enormous impacts of big data on medical stakeholders, patients, physicians, pharmaceutical and medical operators, and healthcare insurers, and also reviews the different challenges that must be taken into account to get the best benefits from all this big data and the available applications.",IEEE Access,21 December 2018,7397 - 7408,2169-3536,,10.1109/ACCESS.2018.2889180,https://doi.org/10.1109/ACCESS.2018.2889180,IEEE
An Integrated Big and Fast Data Analytics Platform for Smart Urban Transportation Management,Sandro Fiore; Donatello Elia; Carlos Eduardo Pires; Demetrio Gomes Mestre; Cinzia Cappiello; Monica Vitali; Nazareno Andrade; Tarciso Braz; Daniele Lezzi; Regina Moraes; Tania Basso; Nádia P. Kozievitch; Keiko Verônica Ono Fonseca; Nuno Antunes; Marco Vieira; Cosimo Palazzo; Ignacio Blanquer; Wagner Meira; Giovanni Aloisio; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ,https://ieeexplore.ieee.org/document/8809689/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8809689,"Smart urban transportation management can be considered as a multifaceted big data challenge. It strongly relies on the information collected into multiple, widespread, and heterogeneous data sources as well as on the ability to extract actionable insights from them. Besides data, full stack (from platform to services and applications) Information and Communications Technology (ICT) solutions need to be specifically adopted to address smart cities challenges. Smart urban transportation management is one of the key use cases addressed in the context of the EUBra-BIGSEA (Europe-Brazil Collaboration of Big Data Scientific Research through Cloud-Centric Applications) project. This paper specifically focuses on the City Administration Dashboard, a public transport analytics application that has been developed on top of the EUBra-BIGSEA platform and used by the Municipality stakeholders of Curitiba, Brazil, to tackle urban traffic data analysis and planning challenges. The solution proposed in this paper joins together a scalable big and fast data analytics platform, a flexible and dynamic cloud infrastructure, data quality and entity matching algorithms as well as security and privacy techniques. By exploiting an interoperable programming framework based on Python Application Programming Interface (API), it allows an easy, rapid and transparent development of smart cities applications.",IEEE Access,22 August 2019,117652 - 117677,2169-3536,,10.1109/ACCESS.2019.2936941,https://doi.org/10.1109/ACCESS.2019.2936941,IEEE
Model-Based Big Data Analytics-as-a-Service: Take Big Data to the Next Level,Claudio Agostino Ardagna; Valerio Bellandi; Michele Bezzi; Paolo Ceravolo; Ernesto Damiani; Cedric Hebert; ; ; ; ; ; ,https://ieeexplore.ieee.org/document/8319508/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8319508,"The Big Data revolution promises to build a data-driven ecosystem where better decisions are supported by enhanced analytics and data management. However, major hurdles still need to be overcome on the road that leads to commoditization and wide adoption of Big Data Analytics (BDA). Big Data complexity is the first factor hampering the full potential of BDA. The opacity and variety of Big Data technologies and computations, in fact, make BDA a failure prone and resource-intensive process, which requires a trial-and-error approach. This problem is even exacerbated by the fact that current solutions to Big Data application development take a bottom-up approach, where the last technology release drives application development. Selection of the best Big Data platform, as well as of the best pipeline to execute analytics, represents then a deal breaker. In this paper, we propose a return to roots by defining a Model-Driven Engineering (MDE) methodology that supports automation of BDA based on model specification. Our approach lets customers declare requirements to be achieved by an abstract Big Data platform and smart engines deploy the Big Data pipeline carrying out the analytics on a specific instance of such platform. Driven by customers' requirements, our methodology is based on an OWL-S ontology of Big Data services and on a compiler transforming OWL-S service compositions in workflows that can be directly executed on the selected platform. The proposal is experimentally evaluated in a real-world scenario focusing on the threat detection system of SAP.",IEEE Transactions on Services Computing,19 March 2018,516 - 529,,,10.1109/TSC.2018.2816941,https://doi.org/10.1109/TSC.2018.2816941,IEEE
A System of Systems Approach for Global Supply Chain Management in the Big Data Era,Tsan-Ming Choi; ,https://ieeexplore.ieee.org/document/8334864/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8334864,"Global supply chain management (GSCM) is increasingly complex and managers find that traditional methods fall short in adequately addressing many associated challenges. As such, it calls for innovative managerial measures. In this paper, we discuss how the system of systems (SoS) approach and big data technologies can be applied to improve GSCM. We first show that the global supply chain is an SoS and examine various principles of the SoS approach. Then, we review various big data related technologies which are commonly employed in global supply chain management. After that, we propose how big data related technologies can be incorporated into the SoS approach to enhance global supply chain operations by presenting an example. This paper provides practitioners a new perspective on how big data related technologies can be used for global supply chain management with an SoS mindset.",IEEE Engineering Management Review,11 April 2018,91 - 97,,,10.1109/EMR.2018.2810069,https://doi.org/10.1109/EMR.2018.2810069,IEEE
A Big Data Provenance Model for Data Security Supervision Based on PROV-DM Model,Yuanzhao Gao; Xingyuan Chen; Xuehui Du; ; ; ,https://ieeexplore.ieee.org/document/9007438/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9007438,"Nowadays, big data has become a hot research topic. It gives fresh impetus to the economic and social development. However, the huge value of big data also makes it the focus of attacks. Big data security incidents occur frequently in recent years. The security supervision capacities for big data do not match its important role. Data provenance which describes the origins of data and the process by which it arrived the current state, is an effective approach for data supervision. For the full use of provenance in big data supervision, a provenance model which defines the concepts used to represent the provenance types and relations is required to be built in advance, but current provenance models do not adapt to big data scenarios well. In this paper, we comprehensively consider the characteristics of big data and the requirements of data security supervision, extend the widely used provenance model PROV-DM by subtyping and new relation definition, and propose a big data provenance model (BDPM) for data supervision. BDPM model supports the provenance representation of various data types and diverse data processing modes to represent the entire data transformation process through different components in the big data system, and defines new relations to enrich provenance analysis functions. Based on BDPM model, we introduce the constraints that should be satisfied in the construction of valid provenance graph and present the data security supervision methods via provenance graph analysis. Finally, we evaluated the satisfiability of BDPM model through a case study.",IEEE Access,24 February 2020,38742 - 38752,2169-3536,,10.1109/ACCESS.2020.2975820,https://doi.org/10.1109/ACCESS.2020.2975820,IEEE
"Big Data Features, Applications, and Analytics in Cardiology—A Systematic Literature Review",Shah Nazir; Muhammad Nawaz; Awais Adnan; Sara Shahzad; Shahla Asadi; ; ; ; ; ,https://ieeexplore.ieee.org/document/8840830/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8840830,"In today's digital world the information surges with the widespread use of the internet and global communication systems. Healthcare systems are also facing digital transformations with the enhancement in the utilization of healthcare information systems, electronic records in medical, wearable, smart devices, handheld devices, and so on. A bulk of data is produced from these digital transformations. The recent increase in medical big data and the development of computational techniques in the field of cardiology enables researchers and practitioners to extract and visualize medical big data in a new spectrum. The role of medical big data in cardiology becomes a challenging task. Early decision making in cardiac healthcare system has massive potential for dropping the cost of care, refining quality of care, and reducing waste and error. Therefore, to facilitate this process a detailed report of the existing literature will be feasible to help the doctors and practitioners in decision making for the purpose of identifying and treating cardiac diseases. This detailed study will summarize results from the existing literature on big data in the field cardiac disease. This research uses the systematic literature protocol as presented by Kitchenham et al. The data was collected from the published materials from 2008 to 2018 as conference or journal publications, books, magazines and other online sources. 190 papers were included relying on the defined inclusion, exclusion, and checking the quality criteria. The current study helped to identify medical big data features, the application of medical big data, and the analytics of the big data in cardiology. The results of the proposed research shows that several studies exist that are associated to medical big data specifically to cardiology. This research summarizes and organizes the existing literature based on the defined keywords and research questions. The analysis will help doctors to make more authentic decisions, whi...",IEEE Access,17 September 2019,143742 - 143771,2169-3536,,10.1109/ACCESS.2019.2941898,https://doi.org/10.1109/ACCESS.2019.2941898,IEEE
Processes Meet Big Data: Connecting Data Science with Process Science,Wil van der Aalst; Ernesto Damiani; ; ,https://ieeexplore.ieee.org/document/7302592/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7302592,"As more and more companies are embracing Big data, it has become apparent that the ultimate challenge is to relate massive amounts of event data to processes that are highly dynamic. To unleash the value of event data, events need to be tightly connected to the control and management of operational processes. However, the primary focus of Big data technologies is currently on storage, processing, and rather simple analytical tasks. Big data initiatives rarely focus on the improvement of end-to-end processes. To address this mismatch, we advocate a better integration of data science, data technology and process science. Data science approaches tend to be process agonistic whereas process science approaches tend to be model-driven without considering the “evidence” hidden in the data. Process mining aims to bridge this gap. This editorial discusses the interplay between data science and process science and relates process mining to Big data technologies, service orientation, and cloud computing.",IEEE Transactions on Services Computing,26 October 2015,810 - 819,,,10.1109/TSC.2015.2493732,https://doi.org/10.1109/TSC.2015.2493732,IEEE
PPHOPCM: Privacy-Preserving High-Order Possibilistic c-Means Algorithm for Big Data Clustering with Cloud Computing,Qingchen Zhang; Laurence T. Yang; Zhikui Chen; Peng Li; ; ; ; ,https://ieeexplore.ieee.org/document/7920374/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920374,"As one important technique of fuzzy clustering in data mining and pattern recognition, the possibilistic c-means algorithm (PCM) has been widely used in image analysis and knowledge discovery. However, it is difficult for PCM to produce a good result for clustering big data, especially for heterogenous data, since it is initially designed for only small structured dataset. To tackle this problem, the paper proposes a high-order PCM algorithm (HOPCM) for big data clustering by optimizing the objective function in the tensor space. Further, we design a distributed HOPCM method based on MapReduce for very large amounts of heterogeneous data. Finally, we devise a privacy-preserving HOPCM algorithm (PPHOPCM) to protect the private data on cloud by applying the BGV encryption scheme to HOPCM, In PPHOPCM, the functions for updating the membership matrix and clustering centers are approximated as polynomial functions to support the secure computing of the BGV scheme. Experimental results indicate that PPHOPCM can effectively cluster a large number of heterogeneous data using cloud computing without disclosure of private data.",IEEE Transactions on Big Data,05 May 2017,25 - 34,,,10.1109/TBDATA.2017.2701816,https://doi.org/10.1109/TBDATA.2017.2701816,IEEE
Privacy-Preserving Data Encryption Strategy for Big Data in Mobile Cloud Computing,Keke Gai; Meikang Qiu; Hui Zhao; ; ; ,https://ieeexplore.ieee.org/document/7931658/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7931658,"Privacy has become a considerable issue when the applications of big data are dramatically growing in cloud computing. The benefits of the implementation for these emerging technologies have improved or changed service models and improve application performances in various perspectives. However, the remarkably growing volume of data sizes has also resulted in many challenges in practice. The execution time of the data encryption is one of the serious issues during the data processing and transmissions. Many current applications abandon data encryptions in order to reach an adoptive performance level companioning with privacy concerns. In this paper, we concentrate on privacy and propose a novel data encryption approach, which is called Dynamic Data Encryption Strategy (D2ES). Our proposed approach aims to selectively encrypt data and use privacy classification methods under timing constraints. This approach is designed to maximize the privacy protection scope by using a selective encryption strategy within the required execution time requirements. The performance of D2ES has been evaluated in our experiments, which provides the proof of the privacy enhancement.",IEEE Transactions on Big Data,18 May 2017,678 - 688,,,10.1109/TBDATA.2017.2705807,https://doi.org/10.1109/TBDATA.2017.2705807,IEEE
An Integrated Framework for Health State Monitoring in a Smart Factory Employing IoT and Big Data Techniques,Wenjin Yu; Yuehua Liu; Tharam Dillon; Wenny Rahayu; Fahed Mostafa; ; ; ; ; ,https://ieeexplore.ieee.org/document/9481251/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9481251,"With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data & AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.",IEEE Internet of Things Journal,12 July 2021,2443 - 2454,,,10.1109/JIOT.2021.3096637,https://doi.org/10.1109/JIOT.2021.3096637,IEEE
Survey of Distributed Computing Frameworks for Supporting Big Data Analysis,,https://ieeexplore.ieee.org/document/10026506/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10026506,"Distributed computing frameworks are the fundamental component of distributed computing systems. They provide an essential way to support the efficient processing of big data on clusters or cloud. The size of big data increases at a pace that is faster than the increase in the big data processing capacity of clusters. Thus, distributed computing frameworks based on the MapReduce computing model are not adequate to support big data analysis tasks which often require running complex analytical algorithms on extremely big data sets in terabytes. In performing such tasks, these frameworks face three challenges: computational inefficiency due to high I/O and communication costs, non-scalability to big data due to memory limit, and limited analytical algorithms because many serial algorithms cannot be implemented in the MapReduce programming model. New distributed computing frameworks need to be developed to conquer these challenges. In this paper, we review MapReduce-type distributed computing frameworks that are currently used in handling big data and discuss their problems when conducting big data analysis. In addition, we present a non-MapReduce distributed computing framework that has the potential to overcome big data analysis challenges.",Big Data Mining and Analytics,26 January 2023,154 - 169,,,10.26599/BDMA.2022.9020014,https://doi.org/10.26599/BDMA.2022.9020014,TUP
CloudFinder: A System for Processing Big Data Workloads on Volunteered Federated Clouds,Abdelmounaam Rezgui; Nickolas Davis; Zaki Malik; Brahim Medjahed; Hamdy S. Soliman; ; ; ; ; ,https://ieeexplore.ieee.org/document/7926341/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7926341,"The proliferation of private clouds that are often underutilized and the tremendous computational potential of these clouds when combined has recently brought forth the idea of volunteer cloud computing (VCC), a computing model where cloud owners contribute underutilized computing and/or storage resources on their clouds to support the execution of applications of other members in the community. This model is particularly suitable to solve big data scientific problems. Scientists in data-intensive scientific fields increasingly recognize that sharing volunteered resources from several clouds is a cost-effective alternative to solve many complex, data- and/or compute-intensive science problems. Despite the promise of the idea of VCC, it still remains at the vision stage at best. Challenges include the heterogeneity and autonomy of member clouds, access control and security, complex inter-cloud virtual machine scheduling, etc. In this paper, we present CloudFinder, a system that supports the efficient execution of big data workloads on volunteered federated clouds (VFCs). Our evaluation of the system indicates that VFCs are a promising cost-effective approach to enable big data science.",IEEE Transactions on Big Data,11 May 2017,347 - 358,,,10.1109/TBDATA.2017.2703830,https://doi.org/10.1109/TBDATA.2017.2703830,IEEE
A Novel Pipeline Approach for Efficient Big Data Broadcasting,Chi-Jen Wu; Chin-Fu Ku; Jan-Ming Ho; Ming-Syan Chen; ; ; ; ,https://ieeexplore.ieee.org/document/7202863/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7202863,"Big-data computing is a new critical challenge for the ICT industry. Engineers and researchers are dealing with data sets of petabyte scale in the cloud computing paradigm. Thus, the demand for building a service stack to distribute, manage, and process massive data sets has risen drastically. In this paper, we investigate the Big Data Broadcasting problem for a single source node to broadcast a big chunk of data to a set of nodes with the objective of minimizing the maximum completion time. These nodes may locate in the same datacenter or across geo-distributed datacenters. This problem is one of the fundamental problems in distributed computing and is known to be NP-hard in heterogeneous environments. We model the Big-data broadcasting problem into a LockStep Broadcast Tree (LSBT) problem. The main idea of the LSBT model is to define a basic unit of upload bandwidth, r, such that a node with capacity c broadcasts data to a set of [c/r] children at the rater. Note that r is a parameter to be optimized as part of the LSBT problem. We further divide the broadcast data into m chunks. These data chunks can then be broadcast down the LSBT in a pipeline manner. In a homogeneous network environment in which each node has the same upload capacity c, we show that the optimal uplink rate r* of LSBT is either c/2 or c/3, whichever gives the smaller maximum completion time. For heterogeneous environments, we present an O(nlog 2 n) algorithm to select an optimal uplink rater* and to construct an optimal LSBT. Numerical results show that our approach performs well with less maximum completion time and lower computational complexity than other efficient solutions in literature.",IEEE Transactions on Knowledge and Data Engineering,14 August 2015,17 - 28,,,10.1109/TKDE.2015.2468714,https://doi.org/10.1109/TKDE.2015.2468714,IEEE
A Deep Learning-Based Data Minimization Algorithm for Fast and Secure Transfer of Big Genomic Datasets,Mohammed Aledhari; Marianne Di Pierro; Mohamed Hefeida; Fahad Saeed; ; ; ; ,https://ieeexplore.ieee.org/document/8290833/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8290833,"In the age of Big Genomics Data, institutions such as the National Human Genome Research Institute (NHGRI) are challenged in their efforts to share volumes of data between researchers, a process that has been plagued by unreliable transfers and slow speeds. These occur due to throughput bottlenecks of traditional transfer technologies. Two factors that affect the efficiency of data transmission are the channel bandwidth and the amount of data. Increasing the bandwidth is one way to transmit data efficiently, but might not always be possible due to resource limitations. Another way to maximize channel utilization is by decreasing the bits needed for transmission of a dataset. Traditionally, transmission of big genomic data between two geographical locations is done using general-purpose protocols, such as hypertext transfer protocol (HTTP) and file transfer protocol (FTP) secure. In this paper, we present a novel deep learning-based data minimization algorithm that 1) minimizes the datasets during transfer over the carrier channels; 2) protects the data from the man-in-the-middle (MITM) and other attacks by changing the binary representation (content-encoding) several times for the same dataset: we assign different codewords to the same character in different parts of the dataset. Our data minimization strategy exploits the alphabet limitation of DNA sequences and modifies the binary representation (codeword) of dataset characters using deep learning-based convolutional neural network (CNN) to ensure a minimum of code word uses to the high frequency characters at different time slots during the transfer time. This algorithm ensures transmission of big genomic DNA datasets with minimal bits and latency and yields an efficient and expedient process. Our tested heuristic model, simulation, and real implementation results indicate that the proposed data minimization algorithm is up to 99 times faster and more secure than the currently used content-encoding scheme used in...",IEEE Transactions on Big Data,13 February 2018,271 - 284,,,10.1109/TBDATA.2018.2805687,https://doi.org/10.1109/TBDATA.2018.2805687,IEEE
Big Privacy: Challenges and Opportunities of Privacy Study in the Age of Big Data,Shui Yu; ,https://ieeexplore.ieee.org/document/7485855/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7485855,"One of the biggest concerns of big data is privacy. However, the study on big data privacy is still at a very early stage. We believe the forthcoming solutions and theories of big data privacy root from the in place research output of the privacy discipline. Motivated by these factors, we extensively survey the existing research outputs and achievements of the privacy field in both application and theoretical angles, aiming to pave a solid starting ground for interested readers to address the challenges in the big data case. We first present an overview of the battle ground by defining the roles and operations of privacy systems. Second, we review the milestones of the current two major research categories of privacy: data clustering and privacy frameworks. Third, we discuss the effort of privacy study from the perspectives of different disciplines, respectively. Fourth, the mathematical description, measurement, and modeling on privacy are presented. We summarize the challenges and opportunities of this promising topic at the end of this paper, hoping to shed light on the exciting and almost uncharted land.",IEEE Access,06 June 2016,2751 - 2763,2169-3536,,10.1109/ACCESS.2016.2577036,https://doi.org/10.1109/ACCESS.2016.2577036,IEEE
A Study on Big Knowledge and Its Engineering Issues,Ruqian Lu; Xiaolong Jin; Songmao Zhang; Meikang Qiu; Xindong Wu; ; ; ; ; ,https://ieeexplore.ieee.org/document/8444740/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8444740,"After entering the big data era, a new term of `big knowledge' has been coined to deal with challenges in mining a mass of knowledge from big data. While researchers used to explore the basic characteristics of big data, we have not seen any studies on the general and essential properties of big knowledge. To fill this gap, this paper studies the concepts of big knowledge, big-knowledge system, and big-knowledge engineering. Ten massiveness characteristics for big knowledge and big-knowledge systems, including massive concepts, connectedness, clean data resources, cases, confidence, capabilities, cumulativeness, concerns, consistency, and completeness, are defined and explored. Based on these characteristics, a comprehensive investigation is conducted on some large-scale knowledge engineering projects, including the Fifth Comprehensive Traffic Survey in Shanghai, the China's Xia-Shang-Zhou Chronology Project, the Troy and Trojan War Project, and the International Human Genome Project, as well as the online free encyclopedia Wikipedia. We also investigate the recent research efforts on knowledge graphs, where they are analyzed to determine which ones can be considered as big knowledge and big-knowledge systems. Further, a definition of big-knowledge engineering and its life cycle paradigm is presented. All of these projects are accordingly checked to determine whether they belong to big-knowledge engineering projects. Finally, the perspectives of big knowledge research are discussed.",IEEE Transactions on Knowledge and Data Engineering,23 August 2018,1630 - 1644,,,10.1109/TKDE.2018.2866863,https://doi.org/10.1109/TKDE.2018.2866863,IEEE
"Big Data Technology: Challenges, Prospects, and Realities",Nitin Singh; Kee-Hung Lai; Markus Vejvar; T. C. E. Cheng; ; ; ; ,https://ieeexplore.ieee.org/document/8658160/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8658160,"We attempt to demonstrate the value of big data to enterprises by interweaving the perceptions, challenges, and opportunities of big data for businesses. While enterprises are aware of the value of big data to their businesses, there are challenges of exploiting big data in terms of data quality and usage. Executives might lack knowledge on how applications are related to one another in the big data ecosystem and the business benefits to reap. We summarize the results of a research study that explores emerging business perceptions of big data. We examine the current practice in 20 large enterprises, each having an annual revenue of more than USD 0.5 billion and present our findings related to executive perceptions. We provide insights on how firms can develop their big data expertise along various dimensions and identify critical ideas to be further investigated to better understand the issues that practitioners and researchers might be equally grappling with.",IEEE Engineering Management Review,04 March 2019,58 - 66,,,10.1109/EMR.2019.2900208,https://doi.org/10.1109/EMR.2019.2900208,IEEE
Developing Big Data Projects in Open University Engineering Courses: Lessons Learned,Juan A. Lara; Aurea Anguera De Sojo; Shadi Aljawarneh; Robert P. Schumaker; Bassam Al-Shargabi; ; ; ; ; ,https://ieeexplore.ieee.org/document/8978544/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8978544,"Big Data courses in which students are asked to carry out Big Data projects are becoming more frequent as a part of University Engineering curriculum. In these courses, instructors and students must face a series of special characteristics, difficulties and challenges that it is important to know about beforehand, so the lecturer can better plan the subject and manage the teaching methods in order to prevent students’ academic dropout and low performance. The goal of this research is to approach this problem by sharing the lessons learned in the process of teaching e-learning courses where students are required to develop a Big Data project as a part of a final degree/master course. In order to do so, a survey was carried out among a group of students enrolled in those kinds of courses during the last years. The quantitative and qualitative analysis of the obtained data led us to present a series of lessons learned that may help other participants (both students and lecturers) to better study, design and teach similar courses. In addition, the results shed light on possible existing open problems in the area of Big Data project development. Both the methodology used and the survey designed in this research were validated by a group of experts in the area using a formal statistical approach at a significance level of p<0.008, which support the validity of the lessons learned.",IEEE Access,03 February 2020,22988 - 23001,2169-3536,,10.1109/ACCESS.2020.2968969,https://doi.org/10.1109/ACCESS.2020.2968969,IEEE
On Big Data Analytics for Greener and Softer RAN,Chih-Lin I; Yunlu Liu; Shuangfeng Han; Sihai Wang; Guangyi Liu; ; ; ; ; ,https://ieeexplore.ieee.org/document/7210136/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7210136,"Big data analytics applied to signaling, traffic, and wireless environment data in mobile communication networks can help realize autonomous network optimization and build big data-based network operation. In this paper, a signaling-based intelligent network optimization scheme is introduced and applied to the current mobile communication networks, such as 4G Long Term Evolution. In 5G era, big data analytics can help mine user and service requirements from the radio access network level, thus allowing a more efficient 5G design and operation. This paper illustrates how it would significantly facilitate local content provision, dynamical network and functionality deployment, user behavior awareness, fine-tuned network operation, and globally optimized energy saving solutions. It is anticipated that the big data-based 5G network design, and the operation will be greener and softer, and better meet the ever increasing user-centric requirements of mobile communication.",IEEE Access,19 August 2015,3068 - 3075,2169-3536,,10.1109/ACCESS.2015.2469737,https://doi.org/10.1109/ACCESS.2015.2469737,IEEE
Accelerated PSO Swarm Search Feature Selection for Data Stream Mining Big Data,Simon Fong; Raymond Wong; Athanasios V. Vasilakos; ; ; ,https://ieeexplore.ieee.org/document/7115942/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7115942,"Big Data though it is a hype up-springing many technical challenges that confront both academic research communities and commercial IT deployment, the root sources of Big Data are founded on data streams and the curse of dimensionality. It is generally known that data which are sourced from data streams accumulate continuously making traditional batch-based model induction algorithms infeasible for real-time data mining. Feature selection has been popularly used to lighten the processing load in inducing a data mining model. However, when it comes to mining over high dimensional data the search space from which an optimal feature subset is derived grows exponentially in size, leading to an intractable demand in computation. In order to tackle this problem which is mainly based on the high-dimensionality and streaming format of data feeds in Big Data, a novel lightweight feature selection is proposed. The feature selection is designed particularly for mining streaming data on the fly, by using accelerated particle swarm optimization (APSO) type of swarm search that achieves enhanced analytical accuracy within reasonable processing time. In this paper, a collection of Big Data with exceptionally large degree of dimensionality are put under test of our new feature selection algorithm for performance evaluation.",IEEE Transactions on Services Computing,01 June 2015,33 - 45,,,10.1109/TSC.2015.2439695,https://doi.org/10.1109/TSC.2015.2439695,IEEE
Can Sensors Collect Big Data? An Energy-Efficient Big Data Gathering Algorithm for a WSN,Shalli Rani; Syed Hassan Ahmed; Rajneesh Talwar; Jyoteesh Malhotra; ; ; ; ,https://ieeexplore.ieee.org/document/7829299/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7829299,"Recently, incredible growth in communication technology has given rise to the hot topic, big data. Distributed wireless sensor networks (WSNs) are the key provider of big data and can generate a significant amount of data. Various technical challenges exist in gathering the real-time data. Energy-efficient routing algorithms can overcome these challenges. The signal transmission features have been obtained by analyzing the experiments. According to these experiments, an energy-efficient big data algorithm (big data efficient gathering, BDEG) for a WSN is proposed for real-time data collection. Clustering communication is established on the basis of a received signal strength indicator and residual energy of sensor nodes. Experimental simulations show that BDEG is stable in terms of the network lifetime and the data transmission time because of the load-balancing scheme. The effectiveness of the proposed scheme is verified through numerical results obtained in MATLAB.",IEEE Transactions on Industrial Informatics,23 January 2017,1961 - 1968,,,10.1109/TII.2017.2656899,https://doi.org/10.1109/TII.2017.2656899,IEEE
"A Survey on Big Data Market: Pricing, Trading and Protection",Fan Liang; Wei Yu; Dou An; Qingyu Yang; Xinwen Fu; Wei Zhao; ; ; ; ; ; ,https://ieeexplore.ieee.org/document/8293785/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8293785,"Big data is considered to be the key to unlocking the next great waves of growth in productivity. The amount of collected data in our world has been exploding due to a number of new applications and technologies that permeate our daily lives, including mobile and social networking applications, and Internet of Thing-based smart-world systems (smart grid, smart transportation, smart cities, and so on). With the exponential growth of data, how to efficiently utilize the data becomes a critical issue. This calls for the development of a big data market that enables efficient data trading. Via pushing data as a kind of commodity into a digital market, the data owners and consumers are able to connect with each other, sharing and further increasing the utility of data. Nonetheless, to enable such an effective market for data trading, several challenges need to be addressed, such as determining proper pricing for the data to be sold or purchased, designing a trading platform and schemes to enable the maximization of social welfare of trading participants with efficiency and privacy preservation, and protecting the traded data from being resold to maintain the value of the data. In this paper, we conduct a comprehensive survey on the lifecycle of data and data trading. To be specific, we first study a variety of data pricing models, categorize them into different groups, and conduct a comprehensive comparison of the pros and cons of these models. Then, we focus on the design of data trading platforms and schemes, supporting efficient, secure, and privacy-preserving data trading. Finally, we review digital copyright protection mechanisms, including digital copyright identifier, digital rights management, digital encryption, watermarking, and others, and outline challenges in data protection in the data trading lifecycle.",IEEE Access,16 February 2018,15132 - 15154,2169-3536,,10.1109/ACCESS.2018.2806881,https://doi.org/10.1109/ACCESS.2018.2806881,IEEE
Big Data Analytics in Mobile Cellular Networks,Ying He; Fei Richard Yu; Nan Zhao; Hongxi Yin; Haipeng Yao; Robert C. Qiu; ; ; ; ; ; ,https://ieeexplore.ieee.org/document/7429688/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7429688,"Mobile cellular networks have become both the generators and carriers of massive data. Big data analytics can improve the performance of mobile cellular networks and maximize the revenue of operators. In this paper, we introduce a unified data model based on the random matrix theory and machine learning. Then, we present an architectural framework for applying the big data analytics in the mobile cellular networks. Moreover, we describe several illustrative examples, including big signaling data, big traffic data, big location data, big radio waveforms data, and big heterogeneous data, in mobile cellular networks. Finally, we discuss a number of open research challenges of the big data analytics in the mobile cellular networks.",IEEE Access,09 March 2016,1985 - 1996,2169-3536,,10.1109/ACCESS.2016.2540520,https://doi.org/10.1109/ACCESS.2016.2540520,IEEE
FDM: Fuzzy-Optimized Data Management Technique for Improving Big Data Analytics,Gunasekaran Manogaran; P. Mohamed Shakeel; S. Baskar; Ching-Hsien Hsu; Seifedine Nimer Kadry; Revathi Sundarasekar; Priyan Malarvizhi Kumar; Bala Anand Muthu; ; ; ; ; ; ; ; ,https://ieeexplore.ieee.org/document/9166621/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9166621,"Big data analytics and processing require complex architectures and sophisticated techniques for extracting useful information from the accumulated information. Visualizing the extracted data for real-time solutions is demanding in accordance with the semantics and the classification employed by the processing models. This article introduces fuzzy-optimized data management (FDM) technique for classifying and improving coalition of accumulated information based semantics and constraints. The dependency of the information is classified on the basis of the relationships modeled between the data based on the attributes. This technique segregates the considered attributes based on similarity index boundaries to process complex data in a controlled time. The performance of the proposed FDM is analyzed using a real-time weather forecast dataset consisting of sensor data (observed) and image data (captured). With this dataset, the functions of FDM such as input semantics analytics and classification based on similarity are performed. The metrics classification and processing time and similarity index are analyzed for the varying data sizes, classification instances, and dataset records. The proposed FDM is found to achieve 36.28% less processing time for varying classification instances, and 12.57% high similarity index.",IEEE Transactions on Fuzzy Systems,13 August 2020,177 - 185,,,10.1109/TFUZZ.2020.3016346,https://doi.org/10.1109/TFUZZ.2020.3016346,IEEE
Advanced Semiconductor Manufacturing Using Big Data,Tomio Tsuda; Shinji Inoue; Akihiro Kayahara; Shin-ichi Imai; Tomoya Tanaka; Naoaki Sato; Satoshi Yasuda; ; ; ; ; ; ; ,https://ieeexplore.ieee.org/document/7123668/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7123668,"This paper describes the development and the actual utilization of fab-wide fault detection and classification (FDC) for the advanced semiconductor manufacturing using big data. In the fab-wide FDC, the collection of equipment's big data for the FDC judgment is required; hence, we developed the equipment monitoring system that handles the data in a superior method in high speed and in real time. We succeeded in stopping equipment and lots automatically when the equipment was detected as fault condition. In addition, we developed the environment that enables immediate data collection for analysis by the data aggregation and merging functions, which extracts keys correlating to yield from the equipment's parameter. Furthermore, we succeeded in development of the high-speed and high-accuracy process control system that implemented virtual metrology and the run-to-run function for the purpose to reduce process variation.",IEEE Transactions on Semiconductor Manufacturing,15 June 2015,229 - 235,,,10.1109/TSM.2015.2445320,https://doi.org/10.1109/TSM.2015.2445320,IEEE
Adaptive Trust Management and Data Process Time Optimization for Real-Time Spark Big Data Systems,Seungwoo Seo; Jong-Moon Chung; ; ,https://ieeexplore.ieee.org/document/9623558/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9623558,"Applications supporting businesses, smart systems, social networks, and advanced video applications such as eXtended Reality (XR) require large amounts of data processing to be provided in real-time. Therefore, the processing speed of big data systems is more important than ever. On the other hand, protecting a big data system is not easy, as various types of nodes and clusters are supported by various wired and wireless networks. Commonly security procedures slow down the response time of big data networks, and therefore, enhanced security and performance speed techniques need to be co-designed into the system. In this paper, a trusted streaming adaptive failure-compensation (TSAF) scheme is proposed that uses a trust management scheme to identify malicious nodes in Spark big data systems, exclude them from job/task processing, and calculate the number of nodes that can satisfy the process’s object completion time. The TSAF scheme shows an improved processing performance when there are attacks on the big data system compared to other existing real-time big data processing schemes. For the case of no security attack, the results show that the processing time of TSAF is faster by about 1 ~ 2% compared to the existing big data processing schemes when the process completion object time is set to 0.5 s. Even when the ratio of malicious nodes performing security attacks on worker nodes reaches 0.5, the results show that TSAF can satisfy over 75% of the tasks within the object time, which is significantly higher compared to the existing big data processing schemes.",IEEE Access,22 November 2021,156372 - 156379,2169-3536,,10.1109/ACCESS.2021.3129885,https://doi.org/10.1109/ACCESS.2021.3129885,IEEE
A Human-in-the-Loop Anomaly Detection Architecture for Big Traffic Data of Cellular Network,Shenglong Liu; Yuxiao Xia; Di Wang; ; ; ,https://ieeexplore.ieee.org/document/10471346/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10471346,"In the era of mobile big data, smart mobile devices have become an integral part of our daily life, which brings many benefits to the digital society. However, their popularity and relatively lax security make them vulnerable to various cyber threats. Traditional network traffic analysis techniques utilizing pattern matching and regular expressions matching algorithms are becoming insufficient for mobile big data. Network traffic anomaly detection is an effective method to replace traditional methods. Network traffic anomaly detection can solve many new challenges brought by future network and protect the security of network. In this article, we propose a streaming network framework for mobile big data, referred to as SNMDF, which provides massive data traffic collection, processing, analysis, and updating functions, to cope with the tremendous amount of data traffic. In particular, by analyzing the specific characteristics of anomaly traffic data from flow and user behavior, our proposed SNMDF demonstrates its capability to offer real data-based advice to address new challenges for future wireless networks from the viewpoints of operators. Tested by real mobile big data, SNMDF has proven its efficiency and reliability. Furthermore, SNMDF is accessed for the digital twin of the space Internet, which validates that it can be generalized to other environments with massive data traffic or big data.",IEEE Access,18 March 2024,41787 - 41797,2169-3536,,10.1109/ACCESS.2024.3376413,https://doi.org/10.1109/ACCESS.2024.3376413,IEEE
Visual Analysis of Multidimensional Big Data: A Scalable Lightweight Bundling Method for Parallel Coordinates,Wenqiang Cui; Girts Strazdins; Hao Wang; ; ; ,https://ieeexplore.ieee.org/document/9599491/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9599491,"Varied edge bundling methods have been used to reduce visual clutter in parallel coordinates plots (PCP). However, existing edge-bundled PCP do not scale well for visual analysis of multidimensional big data and often overplot the bundles in the area near the axes. In this study, we propose a scalable lightweight bundling method to support visual analysis of multidimensional big data in PCP. It helps the users discover trends and detect outliers in the data by bundling the edges between each two adjacent axes independently. We integrate human judgments into the two-dimensional data binning by novel interactions to accelerate the clustering process of the data. We use the frequency-based representation to render the clusters as histogram-like bundles to reveal the distribution of the data and eliminate the overplotting of the bundles. Based on our method, we build a lightweight web-based visual analytics system for exploring multidimensional big data in PCP. The scalability analysis of our method shows that its clustering time increases linearly with the size of the data. Its rendering time is independent of the size of the data. It can cluster and visualize 1 million data records with 6 dimensions in about 1 second in web-based visualization without pre-computation of the data or hardware-accelerated rendering. We conduct two case studies and a user study to compare our method with classic PCP and two state-of-the-art edge-bundled PCP. The results show that our method is more efficient and effective for visually analyzing multidimensional big data.",IEEE Transactions on Big Data,02 November 2021,106 - 117,,,10.1109/TBDATA.2021.3123982,https://doi.org/10.1109/TBDATA.2021.3123982,IEEE
"Big IoT Data Analytics: Architecture, Opportunities, and Open Research Challenges",Mohsen Marjani; Fariza Nasaruddin; Abdullah Gani; Ahmad Karim; Ibrahim Abaker Targio Hashem; Aisha Siddiqa; Ibrar Yaqoob; ; ; ; ; ; ; ,https://ieeexplore.ieee.org/document/7888916/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7888916,"Voluminous amounts of data have been produced, since the past decade as the miniaturization of Internet of things (IoT) devices increases. However, such data are not useful without analytic power. Numerous big data, IoT, and analytics solutions have enabled people to obtain valuable insight into large data generated by IoT devices. However, these solutions are still in their infancy, and the domain lacks a comprehensive survey. This paper investigates the state-of-the-art research efforts directed toward big IoT data analytics. The relationship between big data analytics and IoT is explained. Moreover, this paper adds value by proposing a new architecture for big IoT data analytics. Furthermore, big IoT data analytic types, methods, and technologies for big data mining are discussed. Numerous notable use cases are also presented. Several opportunities brought by data analytics in IoT paradigm are then discussed. Finally, open research challenges, such as privacy, big data mining, visualization, and integration, are presented as future research directions.",IEEE Access,29 March 2017,5247 - 5261,2169-3536,,10.1109/ACCESS.2017.2689040,https://doi.org/10.1109/ACCESS.2017.2689040,IEEE
The Application of Big Data in Enterprise Information Intelligent Decision-Making,Shuangshuang Ying; Hao Liu; ; ,https://ieeexplore.ieee.org/document/9511435/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9511435,"With the continuous increase of mass information in the process of enterprise operation, information redundancy interference poses a challenge to enterprise information decision-making. Therefore, this paper applies big data analysis technology to enterprise information intelligent decision-making, and builds an enterprise information intelligent decision-making model based on big data analysis. The key data of enterprise is mined by using the density weight Canopy to improve the K-Gmedoids algorithm. After sorting, filtering, and transforming, the big data model designed in this paper uses interactive genetic algorithms to obtain the optimal decision-making strategy of enterprise information through experimental tests, which has a significant impact on the decision-making management and the competitiveness of the enterprise. In the process of enterprise information collection and intelligent decision-making, the interactive genetic algorithm generates a 95% match between the optimal business operation plan and the problems that need to be solved in the actual operation of the enterprise, which can better improve the ability of the enterprise to deal with problems. The number of iterations of the optimal decision-making plan obtained by the company is only 6 times, which has a good use effect while improving work efficiency.",IEEE Access,11 August 2021,120274 - 120284,2169-3536,,10.1109/ACCESS.2021.3104147,https://doi.org/10.1109/ACCESS.2021.3104147,IEEE
Redundancy and Complexity Metrics for Big Data Classification: Towards Smart Data,Jesus Maillo; Isaac Triguero; Francisco Herrera; ; ; ,https://ieeexplore.ieee.org/document/9083972/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9083972,"It is recognized the importance of knowing the descriptive properties of a dataset when tackling a data science problem. Having information about the redundancy, complexity and density of a problem allows us to make decisions as to which data preprocessing and machine learning techniques are most suitable. In classification problems, there are multiple metrics to describe the overlapping of the features between classes, class imbalances or separability, among others. However, these metrics may not scale up well when dealing with big datasets, or may not simply be sufficiently informative in this context. In this paper, we provide a package of metrics for big data classification problems. In particular, we propose two new big data metrics: Neighborhood Density and Decision Tree Progression, which study density and accuracy progression by discarding half of the samples. In addition, we enable a number of basic metrics to handle big data. The experimental study carried out in standard big data classification problems shows that our metrics can quickly characterize big datasets. We identified a clear redundancy of information in most datasets, so that, discarding randomly 75% of the samples does not drastically affect the accuracy of the classifiers used. Thus, the proposed big data metrics, which are available as a Spark-Package, provide a fast assessment of the shape of a classification dataset prior to applying big data preprocessing, toward smart data.",IEEE Access,01 May 2020,87918 - 87928,2169-3536,,10.1109/ACCESS.2020.2991800,https://doi.org/10.1109/ACCESS.2020.2991800,IEEE
Big Data Analytics and Mining for Effective Visualization and Trends Forecasting of Crime Data,Mingchen Feng; Jiangbin Zheng; Jinchang Ren; Amir Hussain; Xiuxiu Li; Yue Xi; Qiaoyuan Liu; ; ; ; ; ; ; ,https://ieeexplore.ieee.org/document/8768367/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8768367,"Big data analytics (BDA) is a systematic approach for analyzing and identifying different patterns, relations, and trends within a large volume of data. In this paper, we apply BDA to criminal data where exploratory data analysis is conducted for visualization and trends prediction. Several the state-of-the-art data mining and deep learning techniques are used. Following statistical analysis and visualization, some interesting facts and patterns are discovered from criminal data in San Francisco, Chicago, and Philadelphia. The predictive results show that the Prophet model and Keras stateful LSTM perform better than neural network models, where the optimal size of the training data is found to be three years. These promising outcomes will benefit for police departments and law enforcement organizations to better understand crime issues and provide insights that will enable them to track activities, predict the likelihood of incidents, effectively deploy resources and optimize the decision making process.",IEEE Access,22 July 2019,106111 - 106123,2169-3536,,10.1109/ACCESS.2019.2930410,https://doi.org/10.1109/ACCESS.2019.2930410,IEEE
Big Data Quality Assurance Through Data Traceability: A Case Study of the National Standard Reference Data Program of Korea,Doyoung Lee; ,https://ieeexplore.ieee.org/document/8667300/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8667300,"In the era of big data, the scientific and social demand for quality data is aggressive and urgent. This paper sheds light on the expanded role of metrology of verifying validated procedures of data production and developing adequate uncertainty evaluation methods to ensure the trustworthiness of data and information. In this regard, I explore the mechanism of the national standard reference data (SRD) program of Korea, which connects various scientific and social sectors to metrology by applying useful metrological concepts and methods to produce reliable data and convert such data into national standards. In particular, the changing interpretation of metrological key concepts, such as “measurement,” “traceability,” and “uncertainty,” will be explored and reconsidered from the perspective of data quality assurance. As a result, I suggest the concept of “data traceability” with “the matrix of data quality evaluation” according to the elements of a data production system and related evaluation criteria. To conclude, I suggest social and policy implications for the new role of metrology and standards for producing and disseminating reliable knowledge sources from big data.",IEEE Access,14 March 2019,36294 - 36299,2169-3536,,10.1109/ACCESS.2019.2904286,https://doi.org/10.1109/ACCESS.2019.2904286,IEEE
Eco-Environment Construction of English Teaching Using Artificial Intelligence Under Big Data Environment,Maohua Sun; Yuangang Li; ; ,https://ieeexplore.ieee.org/document/9235580/,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9235580,"Application of big data and artificial intelligence has become one influence factor of English teaching, which have broken the balance of the teaching Eco-environment for English. In this article, the artificial intelligence and big data are introduced into English teaching to propose a new teaching Eco-environment construction method to meet the needs of the social development and international communication in English. In the proposed method, the characteristics of English teaching under big data environment are analyzed in detail. Then the big data technology is used to construct a new Eco-environment of English teaching to improve the teaching and learning quality. The data mining method is one of artificial intelligence methods, which is used to analyze the relationship of interdependence and mutual restriction among various factors in English teaching in order to build and implement a new Eco-environment with the information sharing, quality teaching and personalized learning of English. Finally, through the practical application of the constructed Eco-environment, the experiment results show that the proposed method can help students update their learning concepts, methods and contents of English, inspire their interest and initiative by comparing with some existed teaching methods, so as to improve their learning effects and application ability of English. Therefore, the constructed Eco-environment provides a new idea and direction for English teaching reform by application of big data and artificial intelligence.",IEEE Access,22 October 2020,193955 - 193965,2169-3536,,10.1109/ACCESS.2020.3033068,https://doi.org/10.1109/ACCESS.2020.3033068,IEEE
